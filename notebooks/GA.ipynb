{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Theory of GAs***\n",
    "\n",
    "The first appearance of genetic algorithms (GAs) was on J. D. Bagley's thesis *\"The behavior of adaptive systems which employ genetic and correlative algorithm\"*. He described them like this: *\"[...] genetic algorithms are simulations of evolution, of what kind ever. In most cases, however, genetic algorithms are nothing else than probabilistic optimization methods which are based on the principles of evolutions\"*.\n",
    "\n",
    "In general, the problem GAs are used to solve is to find $x_\\in X$ such that an arbitrary $f:X \\longrightarrow \\mathbb{R}$ is maximal, i.e. $f(x_0)=\\max_{x\\in X}f(x)$. Depending on the actual problem, it can be quite difficult to obtain such point and it might be sufficient to have a local maximum or to be as close as possible to the global maximum. The function $f$ is called *\"fitness\"* because it assigns values to individuals $x\\in X$ so that we can compare them. Reproduction and adaption are carried out at genetic information level, so GAs do not operate on the values of the search space $X$, but on some coded versions of them (like strings or arrays). \n",
    "\n",
    "Assume $S$ to be a set of strings (or arrays) in general with underlining grammar and let $X$ be the search space of the above optimization problem, then the function $c: X \\longrightarrow S$ is called a *coding function*; conversely $c^{-1}: S \\rightarrow X$ is called a *decoding function*.\n",
    "In practice, coding and decoding function have to be specified according to the needs of the actual problem.\n",
    "\n",
    "The transition from one generation to the next consists of four basic components:\n",
    "1. ***selection***: choose the individuals to be reproduced according to their fitness values.\n",
    "2. ***crossover***: merge the genetic information of two individuals (i.e. their chromosomes); if implemented correctly, good parents produce good children.\n",
    "3. ***mutation***: in GA, mutation is realized as a random deformation of the chromosomes arrays with a certain probability. The positive effects are the preservation of genetic diversity and avoiding of local maxima.\n",
    "4. ***sampling***: it's the procedure that computes a new generation from the previous one and its offsprings.\n",
    "\n",
    "Note: normal genetic algorithms do not use any auxiliary information about the objective function such as its gradient, so they can be applied to any kind of continuous or discrete optimization problem. Moreover, since they work on the whole population and not on a single point at time (like GD), they are more robust and have more chance to find the global optimum.\n",
    "\n",
    "---\n",
    "\n",
    "Note: the paper doesn't describe in depth all the parts of the algorithm and the design choices made; we have made them using what we learnt, maybe other choices could be more effective.\n",
    "\n",
    "The starting dataset is made of $F$ features, so $X$ will be the *power set* of $F$: $$X=\\mathcal{P}(F)=\\{A\\,|\\,A\\subseteq F\\}$$\n",
    "Chromosomes are represented as arrays of length $|F|$ ($b = \\{0, 1\\}^{|F|}$) where $i\\text{-th}$ gene represents wether feature $f_i$ is considered by the individual. Coding and decoding functions come naturally:\n",
    "$$\n",
    "c(A) = (b_1, b_2, \\dots, b_{|F|}), \\quad \\text{where } b_i =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } f_i \\in A, \\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c^{-1}(b) = \\{ f_i \\in F \\mid b_i = 1 \\}\n",
    "$$\n",
    "\n",
    "Authors suggested to take as fitness function the accuracy $\\phi$.  \n",
    "\n",
    "At each timestamp $t$, we'll consider a fixed size population with $m$ individuals:\n",
    "$$\\mathcal{B}_t=(b_{1, t}, b_{2, t}, \\dots, b_{m, t})$$\n",
    "\n",
    "The algorithm developed in this notebook is the following:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\textbf{GA}(\\phi_m : \\text{min fitness to reach}\\\\\n",
    "&\\quad \\quad      m: \\text{population size},\\\\\n",
    "&\\quad\\quad      F: \\text{initial set of features}, \\\\\n",
    "&\\quad\\quad      k: \\text{total number of iterations},\\\\\n",
    "&\\quad\\quad      p_m : \\text{probability of mutation},\\\\\n",
    "&\\quad\\quad      p_c : \\text{probability of crossover},\\\\\n",
    "&\\quad\\quad      n: \\text{number of crossover points},\\\\\n",
    "&\\quad\\quad      X_{train}: \\text{matrix with training samples}, \\\\\n",
    "&\\quad\\quad      y_{train}: \\text{vector with training labels}, \\\\\n",
    "&\\quad\\quad      X_{test}: \\text{matrix with testing samples}, \\\\\n",
    "&\\quad\\quad      y_{test}: \\text{vector with testing labels}\n",
    "):\\\\\n",
    "\\\\\n",
    "& \\quad t := 0 \\\\\n",
    "& \\quad \\text{compute initial population  } \\mathcal{B}_0 = (b_{1,0}, \\dots, b_{m,0}) \\\\\n",
    "& \\quad \\text{compute fitness for each individual } b_{i, 0}\\\\\n",
    "& \\quad \\text{compute maximum fitness } \\phi_0 \\text{ for } \\mathcal{B}_0\\\\\n",
    "\\\\\n",
    "& \\quad \\textbf{while  } \\phi_t < \\phi_m \\textbf{ and } t < k: \\\\\n",
    "& \\quad \\quad   \\textbf{for  }i := 0 \\textbf{ to } m - 1: \\\\\n",
    "& \\quad \\quad  \\quad     \\text{select an individual  } b_{i,t+1} \\text{ from  } \\mathcal{B}_t \\\\\n",
    "& \\quad \\quad   \\textbf{for  } i := 0 \\textbf{ to } m-2 \\textbf{ step } 2: \\\\\n",
    "& \\quad \\quad  \\quad     \\textbf{if } \\text{Random}[0,1] \\leq p_c: \\\\\n",
    "& \\quad \\quad  \\quad \\quad         \\text{cross } b_{i,t+1} \\text{ with } b_{i+1,t+1} \\\\\n",
    "& \\quad \\quad   \\textbf{for  } i := 0 \\textbf{ to } m -1: \\\\\n",
    "& \\quad \\quad  \\quad     \\text{eventually mutate } b_{i,t+1} \\\\\n",
    "& \\quad \\quad t := t + 1 \\\\\n",
    "& \\quad \\quad \\text{compute fitness for each individual } b_{i, t}\\\\\n",
    "& \\quad \\quad \\text{compute maximum fitness } \\phi_t \\text{ for } \\mathcal{B}_t\\\\\n",
    "&\\\\\n",
    "& \\quad \\textbf{return } b_{i, t} \\text{ with maximum } \\phi \\text{ and its fitness}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the sampling policy is: replace selected individuals with one of its children and all the others die immediately. The genetic operations are:\n",
    "1. ***selection***: it can be deterministic, but in most implementations it has random components. The one used in this notebook is *\"roulette wheel\"* (or *\"proportional selection\"*) where the probability to choose a certain individual is proportional to its fitness:\n",
    "$$P\\{b_{i, t} \\text{ is selected}\\} = \\frac{\\phi(b_{i, t})}{\\sum_{k = 1}^m\\phi(b_{k, t})}$$\n",
    "\n",
    "It's called *\"roulette\"* because it resembles a roulette game where the slots aren't equally wide.\n",
    "\n",
    "2. ***crossover***: it's the exchange of genes between chromosomes of the two parents. We can realize it using different techniques:\n",
    "    * *single point*: the two vectors are cut at a randomly chosen position and the two tails are swapped.\n",
    "    * *multiple points crossover*: instead of only one, $n$ breaking points are randomly chosen; then, every second section is swapped.\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    &\\textbf{crossover}(p_1:\\text{first parent},\\\\\n",
    "    &\\quad\\quad\\quad\\quad\\quad p_2: \\text{second parent}, \\\\\n",
    "    &\\quad\\quad\\quad\\quad\\quad n: \\text{number of crossover points}):\\\\\n",
    "    &\\text{create two empty arrays $c_1$, $c_2$ for the children}\\\\\n",
    "    &\\text{generate $n$ distinct positions ($P$) in range $[0, m-1]$ and sort them}\\\\\n",
    "    &\\text{swap} := \\text{false}\\\\\n",
    "    &\\text{start} := 0\\\\\n",
    "    &\\\\\n",
    "    &\\textbf{for  } c \\in P \\cup [n]:\\\\\n",
    "    &\\quad \\textbf{if } \\text{swap}:\\\\\n",
    "    &\\quad\\quad c_1[\\text{start} : k] = p_2[\\text{start} : k]\\\\\n",
    "    &\\quad\\quad c_2[\\text{start} : k] = p_1[\\text{start} : k] \\\\  \n",
    "    &\\quad\\textbf{else:}\\\\\n",
    "    &\\quad\\quad c_1[\\text{start} : k] = p_1[\\text{start} : k]\\\\\n",
    "    &\\quad\\quad c_2[\\text{start} : k] = p_2[\\text{start} : k]  \\\\\n",
    "    &\\quad\\text{swap} := \\textbf{not } \\text{swap}    \\\\\n",
    "    &\\quad\\text{start} := c\\\\\n",
    "    &\\\\\n",
    "    &\\textbf{return } c_1, c_2\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "3. ***mutation***: in reality, the probability that a certain gene is mutated is almost equal for all genes, so for the $i\\text{-th}$ gene if $\\text{random}[0, 1] \\leq p_m$ we invert it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***GA algorithm definition***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T14:00:20.570389Z",
     "start_time": "2025-02-20T14:00:19.099486Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.models.RandomForest import *\n",
    "from utils.models.DecisionTree import *\n",
    "from utils.models.NaiveBayes import *\n",
    "from utils.models.ANN import *\n",
    "from utils.models.LogisticRegression import *\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from utils.model_evaluation import *\n",
    "from utils.data_processer import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T14:00:22.590824Z",
     "start_time": "2025-02-20T14:00:22.578522Z"
    }
   },
   "outputs": [],
   "source": [
    "class GeneticAlgorithm:\n",
    "    def __init__(self, min_fit, pop_size, num_features, max_iters, mut_prob, crs_prob, num_crs_points, model, fit_funct = lambda x, y: accuracy(x, y)):\n",
    "        \"\"\"\n",
    "            Initializes the GA object.\n",
    "\n",
    "            Args:\n",
    "                min_fit (float): target fitness threshold.\n",
    "                pop_size (int): population size.\n",
    "                num_features (list): feature set.\n",
    "                max_iters (int): maximum generations.\n",
    "                mut_prob (float): mutation probability.\n",
    "                crs_prob (float): crossover probability.\n",
    "                num_crs_points (int): number of crossover points.\n",
    "                model (tuple): the BaseModel to be instantiated to create the individuals of the population along with a dict containing its parameters.\n",
    "                fit_funct (lambda): the function to be used as fitness with arguments (y_pred, y_true) in this order.\n",
    "        \"\"\"\n",
    "\n",
    "        self.min_fit = min_fit\n",
    "        self.pop_size = pop_size\n",
    "        self.num_features = num_features\n",
    "        self.max_iters = max_iters\n",
    "        self.mut_prob = mut_prob\n",
    "        self.crs_prob = crs_prob\n",
    "        self.num_crs_points = num_crs_points\n",
    "        \n",
    "        #This three attributes provide a lot of flexibility to our code: we can test the GA not only on the RF with accuracy as fitness (as in the paper)\n",
    "        #but also on all the other algorithms we have developed by only instantiating another objet\n",
    "        self.model = model[0]\n",
    "        self.model_params = model[1]\n",
    "\n",
    "        self.fit_funct = fit_funct\n",
    "\n",
    "    def fitness(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "            Computes the fitness of a model by training and testing it on the selected features of the dataset.\n",
    "            The used metric is the accuracy.\n",
    "\n",
    "            Args:\n",
    "                X_train, X_test (np.ndarray): the feature matrix for training and testing.\n",
    "                y_train, y_test (np.ndarray): the target variable for training and testing.\n",
    "\n",
    "            Returns:\n",
    "                fitness: the accuracy value.\n",
    "        \"\"\"\n",
    "\n",
    "        #See the notebook of the model you have passed for a comparison and detailed explanation of how this code works.\n",
    "        #Note: in some cases (like RF) a tradeoff between performance and model capabilities has to be chosen: this might impact on the computed features vector!\n",
    "        inst = self.model(**self.model_params)\n",
    "\n",
    "        # Fit the model\n",
    "        inst.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the target variable\n",
    "        y_pred, _ = inst.predict(X_test)\n",
    "        \n",
    "        return self.fit_funct(y_pred, y_test)\n",
    "\n",
    "    def crossover(self, p1, p2, n):\n",
    "        \"\"\"\n",
    "            This method computes the n-crossover between two parents by following the algorithm described in theory part.\n",
    "\n",
    "            Args:\n",
    "                p1 (np.ndarray): the chromosome of the first parent.\n",
    "                p2 (np.ndarray): the chromosome of te second parent.\n",
    "                n (int): number of split points.\n",
    "            \n",
    "            Returns:\n",
    "                c1 (np.ndarray): the first children.\n",
    "                c2 (np.ndarray): the second children.\n",
    "        \"\"\"\n",
    "\n",
    "        c1 = np.empty(p1.shape)\n",
    "        c2 = np.empty(p2.shape)\n",
    "        crs_pts = np.empty(n, dtype = int)\n",
    "\n",
    "        #Available positions are only the ones from 1 to len(p1) -1\n",
    "        av_pos = np.array(range(1, len(p1) - 1)) \n",
    "\n",
    "        #crs_pts first index\n",
    "        crs_pts[0] = np.random.choice(range(1, len(p1) - 1))\n",
    "\n",
    "        for i in range(n - 1):\n",
    "            #Update available positions by deleting the neighbor positions of the last extracted index: what we want to avoid is a\n",
    "            #situation like [2, 3] that means that no crossover of genes in 2 - 3 will be made.\n",
    "            av_pos = np.setdiff1d(av_pos, np.array(range(crs_pts[i] - 2, crs_pts[i] + 3)))\n",
    "\n",
    "            #Generate another index\n",
    "            crs_pts[i + 1] = np.random.choice(av_pos)\n",
    "\n",
    "        crs_pts = np.sort(crs_pts)\n",
    "        \n",
    "        #We need to append to crs_points the last index of the array, otherwise we wouldn't swap the last genes of the chromosomes!\n",
    "        crs_pts = np.append(crs_pts, len(p1))\n",
    "\n",
    "        swap = False\n",
    "        start = 0\n",
    "\n",
    "        for c in crs_pts:\n",
    "            if swap:\n",
    "                c1[start : c], c2[start : c] = p2[start : c], p1[start : c]\n",
    "            else:\n",
    "                c1[start : c], c2[start : c] = p1[start : c], p2[start : c]\n",
    "\n",
    "            #If one chuck of genes has been swapped, the next should not be swapped and vice versa\n",
    "            swap = not swap\n",
    "\n",
    "            #The new start of the chuck becomes the endpoint of the previous\n",
    "            start = c\n",
    "\n",
    "        return c1, c2\n",
    "\n",
    "    def spawn_threads_fitness(self, B, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "            Each individual can be trained by a different thread to increase performance. \n",
    "            This methods creates a thread pool and assigns a features vector to a spawned thread.\n",
    "\n",
    "            Args:\n",
    "                B (np.ndarray) : the array containing the selected features.\n",
    "                X_train, X_test (np.ndarray): the feature matrix for training and testing.\n",
    "                y_train, y_test (np.ndarray): the target variable for training and testing.\n",
    "            \n",
    "            Returns:\n",
    "                fits: the list of fitness scores.\n",
    "        \"\"\"\n",
    "\n",
    "        #Create a thread pool with at most 20 threads (typically max_workers = min(20, num_cores_of_machine))\n",
    "        with ThreadPoolExecutor(max_workers = 20) as executor:\n",
    "            #To vectorize the function fitness, we need to create a list of tuples, each containing the attributes to be passed to the method\n",
    "            args = [(X_train[:, b.astype(bool)], y_train, X_test[:, b.astype(bool)], y_test) for b in B]\n",
    "\n",
    "            #Then, we use the executor to compute the fitness scores\n",
    "            fits = list(executor.map(lambda p: self.fitness(*p), args)) \n",
    "        \n",
    "        return np.array(list(fits))\n",
    "\n",
    "    def run(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "            Genetic algorithm for feature selection.\n",
    "\n",
    "            Args:\n",
    "                model (BaseModel): the model to be instantiated to create the individuals of the population.\n",
    "\n",
    "            \n",
    "            Returns:\n",
    "                features (np.ndarray): the selected features.\n",
    "                max_fit (float): the fitness value of the solution.\n",
    "                feats_history (list): the history of the feature vectors with highest fitness.\n",
    "                fits_history (list): the history of fitness values.\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(0)\n",
    "        t = 0\n",
    "\n",
    "        #Compute the initial population\n",
    "        B = np.random.choice([0, 1], size = (self.pop_size, self.num_features), p = [0.4, 0.6])\n",
    "\n",
    "        #Compute the fitness of each individual in the initial population and the maximum fit reached\n",
    "        fits = self.spawn_threads_fitness(B, X_train, y_train, X_test, y_test)\n",
    "        max_fit, max_fit_id = np.max(fits), np.argmax(fits)\n",
    "\n",
    "        feats_history = [B[max_fit_id, :]]\n",
    "        fitness_history = [max_fit]\n",
    "\n",
    "        while max_fit < self.min_fit and t < self.max_iters:\n",
    "            if t % 5 == 0:\n",
    "                print(f\"Current generation: {t}\")\n",
    "            \n",
    "            #Start with selection\n",
    "            #Define the \"roulette wheel\" probabilities (note: in this case all fitness measures are positive)\n",
    "            roulette_probs = fits / np.sum(fits)\n",
    "\n",
    "            #Choose randomly m individuals: by how selection probabilities are defined, individuals with higher fitness should be selected more frequently\n",
    "            ids = np.random.choice(range(self.pop_size), size = self.pop_size, replace = True, p = roulette_probs)\n",
    "\n",
    "            #Update the population with the selected individuals\n",
    "            B = B[ids, :]\n",
    "\n",
    "            #Then, apply crossover\n",
    "            for i in range(0, self.pop_size - 1):\n",
    "                #Crossover has to be applied only with p_c probability\n",
    "                if np.random.rand() < self.crs_prob:\n",
    "                    B[i], B[i + 1] = self.crossover(B[i], B[i + 1], self.num_crs_points)\n",
    "\n",
    "            #Finally, apply mutation\n",
    "            for i in range(self.pop_size):\n",
    "                mut_probs = np.random.rand(self.num_features)\n",
    "\n",
    "                #Mutation has to be applied only when the probability on the single gene is < p_m.\n",
    "                #As defined in theory, it inverts the bit\n",
    "                B[i, mut_probs < self.mut_prob] = 1 - B[i, mut_probs < self.mut_prob]\n",
    "\n",
    "            #At the end, compute again the fitness of each individual in the population and the maximum fit reached\n",
    "            fits = self.spawn_threads_fitness(B, X_train, y_train, X_test, y_test)\n",
    "            c_max_fit, c_max_fit_id = np.max(fits), np.argmax(fits)\n",
    "\n",
    "            feats_history.append(B[c_max_fit_id, :])\n",
    "            fitness_history.append(c_max_fit)\n",
    "\n",
    "            t = t + 1\n",
    "\n",
    "        max_fit_id = np.argmax(np.array(fitness_history))\n",
    "        max_fit = np.max(np.array(fitness_history))\n",
    "\n",
    "        return feats_history[max_fit_id], max_fit, feats_history, fitness_history\n",
    "    \n",
    "    def decode(self, c):\n",
    "        \"\"\"\n",
    "            Utility method to decode a chromosome into a list of strings corresponding to the features of the dataset.\n",
    "\n",
    "            Args:\n",
    "                c (np.ndarray): the chromosome to be decoded.\n",
    "            \n",
    "            Returns:\n",
    "                str_version (list): the decoded chromosome in form of a list of strings.\n",
    "        \"\"\"\n",
    "\n",
    "        str_version = []\n",
    "\n",
    "        for i in range(len(c)):\n",
    "            if c[i] == 1:\n",
    "                str_version.append(f\"V{i}\")\n",
    "        \n",
    "        for i in range(len(str_version)):\n",
    "            if str_version[i] == \"V0\":\n",
    "                str_version[i] = \"Time\"\n",
    "            \n",
    "            if str_version[i] == \"V29\":\n",
    "                str_version[i] = \"Amount\"\n",
    "\n",
    "        return str_version\n",
    "    \n",
    "    def encode(self, str_version):\n",
    "        \"\"\"\n",
    "            Utility method to encode a list of strings corresponding to the features of the dataset into a chromosome.\n",
    "\n",
    "            Args:\n",
    "                str_version (list): the list of strings to be encoded.\n",
    "\n",
    "            Returns:\n",
    "                c (np.ndarray): the resulting chromosome.\n",
    "        \"\"\"\n",
    "\n",
    "        c = np.zeros(self.num_features, dtype = int)\n",
    "\n",
    "        for i in range(len(str_version)):\n",
    "            if str_version[i] != \"Amount\" and str_version[i] != \"Time\":\n",
    "                c[int(str_version[i][1:])] = 1\n",
    "                \n",
    "        for i in range(len(str_version)):\n",
    "            if str_version[i] == \"Time\":\n",
    "                c[0] = 1\n",
    "            \n",
    "            if str_version[i] == \"Amount\":\n",
    "                c[-1] = 1\n",
    "\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Usage of the GA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T14:00:30.389156Z",
     "start_time": "2025-02-20T14:00:29.635253Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "data = pd.read_csv(\"../creditcard_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Class\" column is the target variable, so we remove it from the feature matrix and store it in the variable y\n",
    "X = data.drop(columns = [\"Class\"])\n",
    "y = data[\"Class\"]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T14:00:34.505047Z",
     "start_time": "2025-02-20T14:00:34.447349Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "params = {\n",
    "    \"classes\" : np.unique(y_train)\n",
    "}\n",
    "\n",
    "# GA object instantiation\n",
    "ga = GeneticAlgorithm(1.0, 90, X.shape[1], 50, 0.05, 0.7, 2, (GaussianNaiveBayes, params), lambda x, y: accuracy(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: 0\n",
      "Current generation: 5\n",
      "Current generation: 10\n",
      "Current generation: 15\n",
      "Current generation: 20\n",
      "Current generation: 25\n",
      "Current generation: 30\n",
      "Current generation: 35\n",
      "Current generation: 40\n",
      "Current generation: 45\n",
      "Best feature vector: ['Time', 'V4', 'V9', 'V11', 'V16', 'V17', 'V18', 'V24', 'V26', 'V27'], with fitness 0.9988413328183702\n"
     ]
    }
   ],
   "source": [
    "# Sets for Naive Bayes\n",
    "X_train_nb_1, y_train_nb_1 = X_train, y_train\n",
    "\n",
    "# Run the GA\n",
    "best_feats_nb_1, max_fit_nb_1, feats_hist_nb_1, fit_hist_nb_1 = ga.run(X_train_nb_1, y_train_nb_1, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best feature vector: {ga.decode(best_feats_nb_1)}, with fitness {max_fit_nb_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Gaussian Naive Bayes***\n",
    "##### *(with different target fitness threshold and fitness function)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "params = {\n",
    "    \"classes\" : np.unique(y_train)\n",
    "}\n",
    "\n",
    "# GA object instantiation\n",
    "ga = GeneticAlgorithm(3.0, 90, X.shape[1], 50, 0.05, 0.7, 2, (GaussianNaiveBayes, params), lambda x, y: 2.0 * accuracy(x, y) + precision(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation: 0\n",
      "Current generation: 5\n",
      "Current generation: 10\n",
      "Current generation: 15\n",
      "Current generation: 20\n",
      "Current generation: 25\n",
      "Current generation: 30\n",
      "Current generation: 35\n",
      "Current generation: 40\n",
      "Current generation: 45\n",
      "Best feature vector: ['V4', 'V5', 'V6', 'V10', 'V14', 'V15', 'V16', 'V17', 'V18', 'V21', 'V22', 'V26', 'V28', 'Amount'], with fitness 2.8356928249337723\n"
     ]
    }
   ],
   "source": [
    "# Sets for Naive Bayes\n",
    "X_train_nb_2, y_train_nb_2 = X_train, y_train\n",
    "\n",
    "# Run the GA\n",
    "best_feats_nb_2, max_fit_nb_2, feats_hist_nb_2, fit_hist_nb_2 = ga.run(X_train_nb_2, y_train_nb_2, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best feature vector: {ga.decode(best_feats_nb_2)}, with fitness {max_fit_nb_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Random Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "params = {\n",
    "    \"n_trees\" : 20,\n",
    "    \"criterion\" : \"gini\", \n",
    "    \"max_depth\" : 10, \n",
    "    \"min_samples_split\" : 2, \n",
    "    \"min_samples_leaf\" : 1, \n",
    "    \"min_impurity_decrease\" : 0, \n",
    "    \"max_thresholds\" : 20, \n",
    "    \"random_state\" : 0\n",
    "}\n",
    "\n",
    "# GA object instantiation\n",
    "ga = GeneticAlgorithm(1.0, 90, X.shape[1], 15, 0.05, 0.7, 2, (RandomForest, params), lambda x, y: accuracy(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets for Random Forest\n",
    "X_train_rf, y_train_rf = undersample(X_train, y_train, 0, 1 - (sum(y_train) * 300.0 / X_train.shape[0]))\n",
    "\n",
    "# Run the GA\n",
    "best_feats_rf, max_fit_rf, feats_hist_rf, fit_hist_rf = ga.run(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best feature vector: {ga.decode(best_feats_rf)}, with fitness {max_fit_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Decision Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "params = {\n",
    "    \"criterion\" : \"gini\", \n",
    "    \"max_depth\" : 10, \n",
    "    \"min_samples_split\" : 2, \n",
    "    \"min_samples_leaf\" : 1, \n",
    "    \"min_impurity_decrease\" : 1e-7, \n",
    "    \"max_thresholds\" : 20, \n",
    "    \"max_features\" : None,\n",
    "    \"random_state\" : 0\n",
    "}\n",
    "\n",
    "# GA object instantiation\n",
    "ga = GeneticAlgorithm(1.0, 90, X.shape[1], 15, 0.05, 0.7, 2, (DecisionTree, params), lambda x, y: accuracy(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets for Decision Tree\n",
    "X_train_dt, y_train_dt = undersample(X_train, y_train, 0, 0.5)\n",
    "\n",
    "# Run the GA\n",
    "best_feats_dt, max_fit_dt, feats_hist_dt, fit_hist_dt = ga.run(X_train_dt, y_train_dt, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best feature vector: {ga.decode(best_feats_dt)}, with fitness {max_fit_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Artificial Neural Network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T14:04:48.078317Z",
     "start_time": "2025-02-20T14:04:47.666456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230845, 30), (230845, 1), (56962, 30), (56962, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, y_train, _, _, X_test, y_test = data_split(data.to_numpy(), train_size=0.8)\n",
    "\n",
    "# Number of samples to generate\n",
    "n_samples = 3000\n",
    "\n",
    "# Minority: fraudulent samples\n",
    "X_minority = X_train[y_train[:, 0] == 1]\n",
    "\n",
    "# Apply SMOTE (only on the training set)\n",
    "X_smote = fit_resample(X_minority, n_samples, k=15)\n",
    "y_smote = jnp.ones((n_samples, 1))\n",
    "\n",
    "# Add synthetic data to the training set\n",
    "X_train = jnp.concatenate((X_train, X_smote), axis=0)\n",
    "y_train = jnp.concatenate((y_train, y_smote), axis=0)\n",
    "\n",
    "# Normalize training set\n",
    "X_train_normalized, data_train_min, data_train_max = min_max(X_train)\n",
    "\n",
    "# Normalize testing set\n",
    "X_test_normalized, _, _ = min_max(X_test, data_train_min, data_train_max)\n",
    "\n",
    "X_train_normalized.shape, y_train.shape, X_test_normalized.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T14:04:49.122127Z",
     "start_time": "2025-02-20T14:04:49.118935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = dict(\n",
    "    opt_type= 'RMSprop',\n",
    "    epochs=2000,\n",
    "    batch_size=256,\n",
    "    learning_rate=0.001,\n",
    "    decay_rate=0.9,\n",
    "    epsilon=1e-8,\n",
    "    penalization=0.5\n",
    ")\n",
    "\n",
    "# Model parameters\n",
    "params = {\n",
    "    \"layers_size\": [X_train_normalized.shape[1], 30, 20, 1],\n",
    "    \"act_func\": jnp.tanh,\n",
    "    \"out_act_func\": jax.nn.sigmoid,\n",
    "    \"optimizer\": optimizer,\n",
    "}\n",
    "\n",
    "# GA object instantiation\n",
    "ga = GeneticAlgorithm(1.0, 90, X.shape[1], 15, 0.05, 0.7, 2, (ANN, params), lambda x, y: accuracy(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T14:05:16.503378Z",
     "start_time": "2025-02-20T14:05:15.901752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sets for ANN\n",
    "X_train_ann, y_train_ann = X_train_normalized, y_train\n",
    "\n",
    "# Run the GA\n",
    "best_feats_ann, max_fit_ann, feats_hist_ann, fit_hist_ann = ga.run(X_train_ann, y_train_ann, X_test_normalized, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best feature vector: {ga.decode(best_feats_ann)}, with fitness {max_fit_ann}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, y_train, _, _, X_test, y_test = data_split(data.to_numpy(), train_size=0.8)\n",
    "\n",
    "# Number of samples to generate\n",
    "n_samples = 5000\n",
    "\n",
    "# Minority: fraudulent samples\n",
    "X_minority = X_train[y_train[:, 0] == 1]\n",
    "\n",
    "# Apply SMOTE (only on the training set)\n",
    "X_smote = fit_resample(X_minority, n_samples, k=5)\n",
    "y_smote = jnp.ones((n_samples, 1))\n",
    "\n",
    "# Add synthetic data to the training set\n",
    "X_train = jnp.concatenate((X_train, X_smote), axis=0)\n",
    "y_train = jnp.concatenate((y_train, y_smote), axis=0)\n",
    "\n",
    "# Normalize training set\n",
    "X_train_normalized, data_train_min, data_train_max = min_max(X_train)\n",
    "\n",
    "# Normalize testing set\n",
    "X_test_normalized, _, _ = min_max(X_test, data_train_min, data_train_max)\n",
    "\n",
    "X_train_normalized.shape, y_train.shape, X_test_normalized.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = dict(\n",
    "    opt_type = 'RMSprop',\n",
    "    epochs=7500,\n",
    "    batch_size=4096,\n",
    "    learning_rate=0.001,\n",
    "    decay_rate=0.9,\n",
    "    epsilon=1e-8,\n",
    "    penalization=0.5\n",
    ")\n",
    "\n",
    "# Model parameters\n",
    "params = {\n",
    "    \"optimizer\": optimizer\n",
    "}\n",
    "\n",
    "# GA object instantiation\n",
    "ga = GeneticAlgorithm(1.0, 90, X.shape[1], 15, 0.05, 0.7, 2, (LogisticRegression, params), lambda x, y: accuracy(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets for Logistic Regression\n",
    "X_train_lr, y_train_lr = X_train_normalized, y_train\n",
    "\n",
    "# Run the GA\n",
    "best_feats_lr, max_fit_lr, feats_hist_lr, fit_hist_lr = ga.run(X_train_lr, y_train_lr, X_test_normalized, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best feature vector: {ga.decode(best_feats_lr)}, with fitness {max_fit_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6, figsize = (20, 5))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "axs[0].plot(fit_hist_nb_1, \"o--\")\n",
    "axs[0].set_title(\"NB fitness history\")\n",
    "axs[0].set_xlabel(\"Generation\")\n",
    "axs[0].set_ylabel(\"Fitness (accuracy)\")\n",
    "\n",
    "axs[1].plot(fit_hist_nb_2, \"o--\")\n",
    "axs[1].set_title(\"NB fitness history\")\n",
    "axs[1].set_xlabel(\"Generation\")\n",
    "axs[1].set_ylabel(\"Fitness (accuracy & precision)\")\n",
    "\n",
    "axs[2].plot(fit_hist_rf, \"o--\")\n",
    "axs[2].set_title(\"RF fitness history\")\n",
    "axs[2].set_xlabel(\"Generation\")\n",
    "axs[2].set_ylabel(\"Fitness (accuracy & precision)\")\n",
    "\n",
    "axs[3].plot(fit_hist_dt, \"o--\")\n",
    "axs[3].set_title(\"DT fitness history\")\n",
    "axs[3].set_xlabel(\"Generation\")\n",
    "axs[3].set_ylabel(\"Fitness (accuracy)\")\n",
    "\n",
    "axs[4].plot(fit_hist_ann, \"o--\")\n",
    "axs[4].set_title(\"ANN fitness history\")\n",
    "axs[4].set_xlabel(\"Generation\")\n",
    "axs[4].set_ylabel(\"Fitness (accuracy)\")\n",
    "\n",
    "axs[5].plot(fit_hist_lr, \"o--\")\n",
    "axs[5].set_title(\"LR fitness history\")\n",
    "axs[5].set_xlabel(\"Generation\")\n",
    "axs[5].set_ylabel(\"Fitness (accuracy)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
