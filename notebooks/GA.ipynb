{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Theory of GAs***\n",
    "\n",
    "The first appearence of genetic algorithms (GAs) was on J. D. Bagley's thesis *\"The behaviour of adaptive systems which employ genetic and correlative algorithm\"*. He described them like this: *\"[...] genetic algorithms are simulations of eveolution, of what kind ever. In most cases, however, genetic algorithms are nothing else than probabilistic optimisation methods which are based on the principles of evolutions\"*.\n",
    "\n",
    "In general, the problem GAs are used to solve is to find an $x_\\in X$ such that an arbitrary $f:X \\longrightarrow \\mathbb{R}$ is maximal, i.e. $f(x_0)=\\max_{x\\in X}f(x)$. Depending on the actual problem, it can be quite difficult to obtain such point and it might be sufficent to have a local maximum or to be as close as possible to the global maximum. The function $f$ is called *\"fitness\"* becuase it assigns values to individuals $x\\in X$ so that we can compare them. Reproduction and adaption are carried out at genetic information level, so GAs do not operate on the values of the search space $X$, but on some coded versions of them (like strings or arrays). In our case\n",
    "\n",
    "Assume $S$ to be a set of strings (or arrays) in general with underlyning grammar and let $X$ be the search space of the above optimization problem, then the function:\n",
    "$$c: X \\longrightarrow S$$\n",
    "is called a *conding function*; conversely:\n",
    "$$c^{-1}: S \\rightarrow X$$\n",
    "is called a *decoding function*.\n",
    "In practice, coding and decoding function have to be specified according to the needs of the actual problem.\n",
    "\n",
    "The transition from one generation to the next consists of four basic coponents:\n",
    "1. ***selection***: choose the individuals to be reproduced according to their fitness values.\n",
    "2. ***crossover***: merge the genetic information of two individuals (i.e. their chromosomes); if implemented correctly, good paranets produce good children.\n",
    "3. ***mutation***: in GA, mutation is realized as a random deformation of the chromosone array with a certain probability. The positive effects are the preservation of genetic diversity and avoiding of local maxima.\n",
    "4. ***sampling***: it's the procedure that computes a new generation from the previous one and its offsprings.\n",
    "\n",
    "Note: normal genetic algorithms do not use any auxiliary information about the objective function such as its gradient, so they can be applied to any kind of continuous or discrete optimization problem. Moreover, since they work on the whole population and not on a single point at time (like GD), they are more robust and have more chance to find the global optimum.\n",
    "\n",
    "---\n",
    "\n",
    "Note: the paper doesn't describe in depth all the parts of the notebook and the design choices made; we have made them using what we learnt, maybe other choices could be more effective.\n",
    "\n",
    "The starting dataset is made of $F$ features, so $X$ will be the *power set* of $F$: $$X=\\mathcal{P}(F)=\\{A\\,|\\,A\\subseteq F\\}$$\n",
    "Chromosomes are represented as array of length $|F|$ ($b = \\{0, 1\\}^{|F|}$) where $i\\text{-th}$ gene represents wheter feature $f_i$ is considered by the individual. Coding and decoding functions come naturally:\n",
    "$$\n",
    "c(A) = (b_1, b_2, \\dots, b_{|F|}), \\quad \\text{where } b_i =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } f_i \\in A, \\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c^{-1}(b) = \\{ f_i \\in F \\mid b_i = 1 \\}\n",
    "$$\n",
    "\n",
    "Authors suggested to take as fiteness function $\\phi$ the accuracy.  \n",
    "\n",
    "At each timestamp $t$, we'll consider a fixed size population with $m$ individuals:\n",
    "$$\\mathcal{B}_t=(b_{1, t}, b_{2, t}, \\dots, b_{m, t})$$\n",
    "\n",
    "The algorithm developed in this notebook is the following:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\textbf{GA}(\\phi_m : \\text{min fitness to reach}\\\\\n",
    "&\\quad \\quad      m: \\text{population size},\\\\\n",
    "&\\quad\\quad      F: \\text{initial set of features}, \\\\\n",
    "&\\quad\\quad      k: \\text{total number of iterations},\\\\\n",
    "&\\quad\\quad      p_m : \\text{probability of mutation},\\\\\n",
    "&\\quad\\quad      p_c : \\text{probability of crossover},\\\\\n",
    "&\\quad\\quad      n: \\text{number of crossover points},\\\\\n",
    "&\\quad\\quad      \\phi_T: \\text{threshold on the increment of fitness},\\\\\n",
    "&\\quad\\quad      X_{train}: \\text{matrix with training samples}, \\\\\n",
    "&\\quad\\quad      y_{train}: \\text{vector with training labels}, \\\\\n",
    "&\\quad\\quad      X_{test}: \\text{matrix with testing samples}, \\\\\n",
    "&\\quad\\quad      y_{test}: \\text{vector with testing labels}\n",
    "):\\\\\n",
    "\\\\\n",
    "& \\quad t := 0 \\\\\n",
    "& \\quad \\text{compute initial population:  } \\mathcal{B}_0 = (b_{1,0}, \\dots, b_{m,0}) \\\\\n",
    "& \\quad \\text{compute fitness for each individual } b_{i, 0}\\\\\n",
    "& \\quad \\text{compute maximum fitness } \\phi_0 \\text{ for } \\mathcal{B}_0\\\\\n",
    "\\\\\n",
    "& \\quad \\textbf{while  } \\phi_t < \\phi_m \\textbf{ and } t < k: \\\\\n",
    "& \\quad \\quad   \\textbf{for  }i := 0 \\textbf{ to } m - 1: \\\\\n",
    "& \\quad \\quad  \\quad     \\text{select an individual  } b_{i,t+1} \\text{ from  } \\mathcal{B}_t \\\\\n",
    "& \\quad \\quad   \\textbf{for  } i := 0 \\textbf{ to } m-2 \\textbf{ step } 2: \\\\\n",
    "& \\quad \\quad  \\quad     \\textbf{if } \\text{Random}[0,1] \\leq p_c: \\\\\n",
    "& \\quad \\quad  \\quad \\quad         \\text{cross } b_{i,t+1} \\text{ with } b_{i+1,t+1} \\\\\n",
    "& \\quad \\quad   \\textbf{for  } i := 0 \\textbf{ to } m -1: \\\\\n",
    "& \\quad \\quad  \\quad     \\text{eventually mutate } b_{i,t+1} \\\\\n",
    "& \\quad \\quad t := t + 1 \\\\\n",
    "& \\quad \\quad \\text{compute fitness for each individual } b_{i, t}\\\\\n",
    "& \\quad \\quad \\text{compute maximum fitness } \\phi_t \\text{ for } \\mathcal{B}_t\\\\\n",
    "& \\quad\\quad  \\textbf{if } \\phi_t - \\phi_{t-1} < \\phi_T:\\\\\n",
    "&\\quad\\quad\\quad \\textbf{break}\\\\\n",
    "&\\\\\n",
    "& \\quad \\textbf{return } b_{i, t} \\text{ with maximum } \\phi \\text{ and its fitness}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the sampling policy is: replace selected individuals with one of its children and all the others die immediately. The genetic operations are:\n",
    "1. ***selection***: it can be deterministic, but in most implmenetations it has random components. The one used in this notebook is *\"roulette wheel\"* (or *\"proportional selection\"*) where the probability to choose a certain individual is proportional to its fitness:\n",
    "$$P\\{b_{i, t} \\text{ is selcted}\\} = \\frac{\\phi(b_{i, t})}{\\sum_{k = 1}^m\\phi(b_{k, t})}$$\n",
    "\n",
    "It's called *\"roulette\"* because it resembles a roulette game where the slots aren't equally wide.\n",
    "\n",
    "2. ***crossover***: it's the exchange of genes between chromosomes of the two parents. We can realize it using different techniques:\n",
    "    * *single point*: the two vectors are cut at a randomly chosen position and the two tails are swapped\n",
    "    * *multiple points crossover*: instead of only one, $n$ breaking points are randomly; then, every second section is swapped.\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    &\\textbf{crossover}(p_1:\\text{first parent},\\\\\n",
    "    &\\quad\\quad\\quad\\quad\\quad p_2: \\text{second parent}, \\\\\n",
    "    &\\quad\\quad\\quad\\quad\\quad n: \\text{number of crossover points}):\\\\\n",
    "    &\\text{create two empty arrays $c_1$, $c_2$ for the children}\\\\\n",
    "    &\\text{generate $n$ distinct positions ($P$) in range $[0, m-1]$ and sort them}\\\\\n",
    "    &\\text{swap} := \\text{false}\\\\\n",
    "    &\\text{start} := 0\\\\\n",
    "    &\\\\\n",
    "    &\\textbf{for  } c \\in P \\cup [n]:\\\\\n",
    "    &\\quad \\textbf{if } \\text{swap}:\\\\\n",
    "    &\\quad\\quad c_1[\\text{start} : k] = p_2[\\text{start} : k]\\\\\n",
    "    &\\quad\\quad c_2[\\text{start} : k] = p_1[\\text{start} : k] \\\\  \n",
    "    &\\quad\\textbf{else:}\\\\\n",
    "    &\\quad\\quad c_1[\\text{start} : k] = p_1[\\text{start} : k]\\\\\n",
    "    &\\quad\\quad c_2[\\text{start} : k] = p_2[\\text{start} : k]  \\\\\n",
    "    &\\quad\\text{swap} := \\textbf{not } \\text{swap}    \\\\\n",
    "    &\\quad\\text{start} := c\\\\\n",
    "    &\\\\\n",
    "    &\\textbf{return } c_1, c_2\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "3. ***mutation***: in reality, the probability that a certain gene is mutated is almost equal for all genes, so for the $i\\text{-th}$ gene if $\\text{random}[0, 1] \\leq p_m$ we invert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.models.RandomForest import *\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "        Cmputes the fitness of a RF trained on the given features, by training and testing it.\n",
    "        The used metric is the accuracy.\n",
    "\n",
    "        Args:\n",
    "            X_train, X_test (np.ndarray): the feature matrix for training and testing.\n",
    "            y_train, y_test (np.ndarray): the target variable for training and testing.\n",
    "\n",
    "        Returns:\n",
    "            fitness: the accuracy value.\n",
    "    \"\"\"\n",
    "\n",
    "    #A tradeoff between performance and model capabilities has been chosen: this might impact on the computed features vector;\n",
    "    #see the nootebook on RF for a comparison and detailed explaination of how this code works.\n",
    "    rf = RandomForest(n_trees = 10,\n",
    "                      criterion = \"gini\", \n",
    "                      max_depth = 8, \n",
    "                      min_samples_split = 3, \n",
    "                      min_samples_leaf = 2, \n",
    "                      min_impurity_decrease = 0.0,\n",
    "                      max_thresholds = 20, \n",
    "                      random_state = 0)\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred, _ = rf.predict(X_test)\n",
    "\n",
    "    #We could have used also model_evaluation.compute_metrics(...)[\"accuracy\"]: to speed up, we'll use only this.\n",
    "    tp = np.sum((y_pred == 1) & (y_test == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_test == 0))\n",
    "\n",
    "    return (tp + tn) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(p1, p2, n):\n",
    "    \"\"\"\n",
    "        This method computes the n-crossover between two parents by following the algorithm described in theory part.\n",
    "\n",
    "        Args:\n",
    "            p1 (np.ndarray): the chromosome of the first parent.\n",
    "            p2 (np.ndarray): the chromosome of te second parent.\n",
    "            n (int): number of split points.\n",
    "        \n",
    "        Returns:\n",
    "            c1 (np.ndarray): the first children.\n",
    "            c2 (np.ndarray): the second children.\n",
    "    \"\"\"\n",
    "\n",
    "    c1 = np.empty(p1.shape)\n",
    "    c2 = np.empty(p2.shape)\n",
    "    crs_pts = np.empty(n, dtype = int)\n",
    "\n",
    "    #Available positions are only the ones from 1 to len(p1) -1\n",
    "    availables = np.array(range(1, len(p1) - 1)) \n",
    "\n",
    "    #crs_pts first index\n",
    "    crs_pts[0] = np.random.choice(range(1, len(p1) - 1))\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        #Update available positions by deleting the neighbor positions of the last extracted: what we want to avoid is a\n",
    "        #situation like [2, 3] that means that no crossover of genes in 2 - 3 will be made.\n",
    "        availables = np.setdiff1d(availables, np.array([crs_pts[i] - 1, crs_pts[i], crs_pts[i] + 1]))\n",
    "\n",
    "        #Generate another index\n",
    "        crs_pts[i + 1] = np.random.choice(availables)\n",
    "\n",
    "    crs_pts = np.sort(crs_pts)\n",
    "    \n",
    "    #We need to append to crs_points the last index of the array, otherwise we wouldn't swap the last genes of the chromosomes!\n",
    "    crs_pts = np.append(crs_pts, len(p1))\n",
    "\n",
    "    swap = False\n",
    "    start = 0\n",
    "\n",
    "    for c in crs_pts:\n",
    "        if swap:\n",
    "            c1[start : c], c2[start : c] = p2[start : c], p1[start : c]\n",
    "        else:\n",
    "            c1[start : c], c2[start : c] = p1[start : c], p2[start : c]\n",
    "\n",
    "        #If one chuck of genes has been swapped, the next should not be swapped and viceversa\n",
    "        swap = not swap\n",
    "\n",
    "        #The new start of the chuck becomes the endpoint of the previous\n",
    "        start = c\n",
    "\n",
    "    return c1, c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_threads_fitness(B, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "        Each RF can be trained by a different thread to increase performance. \n",
    "        This methods creates a thread pool and assigns a features vector to a spwaned thred.\n",
    "\n",
    "        Args:\n",
    "            B (np.ndarray) : the array containing the selected features.\n",
    "            X_train, X_test (np.ndarray): the feature matrix for training and testing.\n",
    "            y_train, y_test (np.ndarray): the target variable for training and testing.\n",
    "        \n",
    "        Returns:\n",
    "            fits: the list of fitness scores.\n",
    "    \"\"\"\n",
    "\n",
    "    #Create a thread pool with at most 20 threads (typically max_workers = min(20, num-cores_of_machine))\n",
    "    with ThreadPoolExecutor(max_workers = 20) as executor:\n",
    "        #To vectorize the function fitness, we need to create a list of tuples, each containing the attributes to be passed to the method\n",
    "        args = [(X_train[:, b.astype(bool)], y_train, X_test[:, b.astype(bool)], y_test) for b in B]\n",
    "        \n",
    "        #Then, we use the exeutor to compute the fitnes scores\n",
    "        fits = list(executor.map(lambda p: fitness(*p), args)) \n",
    "        \n",
    "        #When an executor has finished, we shutdown it\n",
    "        executor.shutdown()\n",
    "    \n",
    "    return np.array(list(fits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(phi_m, m, F, k, p_m, p_c, n, threshold, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "        Genetic algorithm for feature selection.\n",
    "\n",
    "        Args:\n",
    "            phi_m (float): target fitness threshold.\n",
    "            m (int): population size.\n",
    "            F (list): feature set.\n",
    "            k (int): maximum generations.\n",
    "            p_m (float): mutation probability.\n",
    "            p_c (float): crossover probability.\n",
    "            n (int): number of crossover points.\n",
    "            threshold (float): threshold on the increment of fitness.\n",
    "            X_train, y_train, X_test, y_test (np.ndarray): training and test data.\n",
    "        \n",
    "        Returns:\n",
    "            features (np.ndarray): the selected features.\n",
    "            max_fit (float): the fitness value of the solution.\n",
    "            feats_history (list): the history of the feature vectors with highest fitness.\n",
    "            fits_history (list): the history of fitness values.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(0)\n",
    "    t = 0\n",
    "\n",
    "    #Compute the initial population\n",
    "    B = np.random.choice([0, 1], size = (m, len(F)))\n",
    "\n",
    "    #Compute the fitness of each individual in the initial population and the maximum fit reached\n",
    "    fits = spawn_threads_fitness(B, X_train, y_train, X_test, y_test)\n",
    "    max_fit, max_fit_id = np.max(fits), np.argmax(fits)\n",
    "\n",
    "    feats_history = [B[max_fit_id, :]]\n",
    "    fitness_history = [max_fit]\n",
    "\n",
    "    #for i in range(m):\n",
    "    #    print(f\"{B[i, :]}   {fits[i]}\")\n",
    "\n",
    "    while max_fit < phi_m and t < k:\n",
    "        print(f\"Currect generation: {t}; current fitness: {max_fit}\")\n",
    "        \n",
    "        #Start with selection\n",
    "        #Define the \"roulette wheel\" probabilities (note: in this case all fitness measures are positive)\n",
    "        roulette_probs = fits / np.sum(fits)\n",
    "\n",
    "        #Choose randomly m individuals: by how selection probabilities are defined, individuals with higher fitness should be selected more more frequently\n",
    "        ids = np.random.choice(range(m), size = m, replace = True, p = roulette_probs)\n",
    "\n",
    "        #Update the population with the selected individuals\n",
    "        B = B[ids, :]\n",
    "\n",
    "        #Then, apply crossover\n",
    "        for i in range(0, m - 1):\n",
    "            #Crossover is to be applied only with p_c probability\n",
    "            if np.random.rand() < p_c:\n",
    "                B[i], B[i + 1] = crossover(B[i], B[i + 1], n)\n",
    "\n",
    "        #Finally, apply mutation\n",
    "        for i in range(m):\n",
    "            mut_probs = np.random.rand(len(F))\n",
    "\n",
    "            #Mutation has to be applied only when the probability on the sigle gene is < p_m\n",
    "            #As defined in theory, it inverts the bit\n",
    "            B[i, mut_probs < p_m] = 1 - B[i, mut_probs < p_m]\n",
    "\n",
    "        #At the end, compute again the fitness of each individual in the initial population and the maximum fit reached\n",
    "        fits = spawn_threads_fitness(B, X_train, y_train, X_test, y_test)\n",
    "        max_fit, max_fit_id = np.max(fits), np.argmax(fits)\n",
    "        \n",
    "        #Terminating condition\n",
    "        if max_fit - fitness_history[-1] < threshold:\n",
    "            break\n",
    "\n",
    "        feats_history.append(B[max_fit_id, :])\n",
    "        fitness_history.append(max_fit)\n",
    "\n",
    "        t = t + 1\n",
    "\n",
    "    return B[max_fit_id], max_fit, feats_history, fitness_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Usage of the GA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.preprocessing import undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "data = pd.read_csv(\"../creditcard_2021.csv\")\n",
    "\n",
    "#\"Class\" column is the target variable, so we remove it from the feature matrix and store it in the variable y\n",
    "X = data.drop(columns = [\"Class\"])\n",
    "y = data[\"Class\"]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "X_train, y_train = undersample(X_train, y_train, 0, 1 - (sum(y_train) * 300.0 / X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currect generation: 0; current fitness: 0.9994967405170698\n",
      "Currect generation: 1; current fitness: 0.9995318516437859\n"
     ]
    }
   ],
   "source": [
    "best_feats, max_fit, feats_hist, fit_hist = genetic_algorithm(1.0, 90, range(X.shape[1]), 100, 0.01, 0.85, 2, 1e-8, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V2', 'V5', 'V7', 'V10', 'V12', 'V14', 'V16', 'V17', 'V19', 'V21', 'V24', 'V28']\n"
     ]
    }
   ],
   "source": [
    "def decoder(c):\n",
    "    str_version = []\n",
    "\n",
    "    for i in range(len(c)):\n",
    "        if c[i] == 1:\n",
    "            str_version.append(f\"V{i}\")\n",
    "    \n",
    "    for i in range(len(c)):\n",
    "        if c[i] == \"V0\":\n",
    "            c[i] = \"Time\"\n",
    "        \n",
    "        if c[i] == \"V29\":\n",
    "            c[i] = \"Amount\"\n",
    "\n",
    "    return str_version\n",
    "\n",
    "print(decoder(best_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: ['V1', 'V5', 'V7', 'V8', 'V11', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'Amount']\n",
      "v2: ['V1', 'V6', 'V13', 'V16', 'V17', 'V16', 'V17', 'V22', 'V23', 'V28', 'Amount']\n",
      "v3: ['V2', 'V11', 'V12', 'V13', 'V15', 'V16', 'V17', 'V18', 'V20', 'V21', 'V24', 'V26', 'Amount']\n",
      "v4: ['V2', 'V7', 'V10', 'V13', 'V15', 'V17', 'V19', 'V28', 'V17', 'Amount']\n",
      "v5: ['Time', 'V1', 'V7', 'V8', 'V9', 'V11', 'V12', 'V14', 'V15', 'V22', 'V27', 'V28', 'Amount']\n",
      "v6: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "v7: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "source": [
    "import utils.model_evaluation as me\n",
    "\n",
    "for name in me.feature_vectors.keys():\n",
    "    print(f\"{name}: {me.feature_vectors[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results shw that the GA is working as expected, but it's not finding any features vector contained into me.feature_vectors. Still, the result appears to be similar\n",
    "#to v4 ('V2', 'V7', 'V10', 'V17', 'V19', 'V28') and v1 ('V5', 'V7', 'V14', 'V16', 'V17', 'V19', 'V21', 'V24')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
