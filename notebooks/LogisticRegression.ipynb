{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T14:55:58.957248Z",
     "start_time": "2025-02-02T14:55:58.953637Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.oversampling import fit_resample\n",
    "from utils.data_processer import *"
   ],
   "outputs": [],
   "execution_count": 454
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T14:56:00.607849Z",
     "start_time": "2025-02-02T14:55:59.763738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"../creditcard_2021.csv\")\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "print(f\"Number of fraudolent transaction: {(data['Class'] == 1).sum()}\")\n",
    "print(f\"Ratio of fraudolent transaction: {data['Class'].mean()}\")"
   ],
   "id": "6e03c2571bf11f0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 284807\n",
      "Number of fraudolent transaction: 492\n",
      "Ratio of fraudolent transaction: 0.001727485630620034\n"
     ]
    }
   ],
   "execution_count": 455
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\n",
    "\n",
    "$$z = \\alpha_0 + \\alpha_1X_1 + \\alpha_2X_2 + ... + \\alpha_nX_n$$\n",
    "\n",
    "$$y_{pred} = sigmoid(q) = \\frac{1}{1 + e^{-q}}$$"
   ],
   "id": "abb0d306a5203aa6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T14:56:01.564358Z",
     "start_time": "2025-02-02T14:56:01.549001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, num_features=None):\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def initialize_parameters(self, num_features=None):\n",
    "        \"\"\"\n",
    "            Sets the parameters (weights and bias) for the logistic regression given the number of features\n",
    "\n",
    "            Returns the parameters of the logistic regression given the number of\n",
    "            neurons of its layers, namely it sets the vector of weights and the bias, initialized randomly\n",
    "\n",
    "            Parameters:\n",
    "            num_features: integer - number of features of the considered dataset\n",
    "\n",
    "            Returns:\n",
    "            weights: ndarray - vector of weights\n",
    "            bias: float - bias term\n",
    "\n",
    "            Raises:\n",
    "            exception: if num_features are not provided\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "        if self.num_features is None:\n",
    "            raise Exception(\"Number of features must be provided\")\n",
    "\n",
    "        np.random.seed(0)  # For reproducibility\n",
    "\n",
    "        # Parameters\n",
    "        weights = np.random.randn(self.num_features, 1)\n",
    "        bias = 0.\n",
    "\n",
    "        return weights, bias\n",
    "\n",
    "    # Loss functions\n",
    "    def cross_entropy(self):\n",
    "        @jax.jit\n",
    "        def callable(x=None, y=None, weights=None, bias=None):\n",
    "            \"\"\"\n",
    "                Computes the Cross Entropy Cost function\n",
    "\n",
    "                Parameters:\n",
    "                x: ndarray - input of the logistic regression\n",
    "                y: ndarray - correct value of the output, one-hot representation\n",
    "                weights: ndarray - vector of weights\n",
    "                bias: float - bias term\n",
    "\n",
    "                Returns:\n",
    "                float - Cross Entropy Cost between the predictions of the logistic regression and the correct values\n",
    "\n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if weights and bias are not provided\n",
    "            \"\"\"\n",
    "\n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if weights is None or bias is None:\n",
    "                raise Exception(\"params (weights and/or bias) are not provided\")\n",
    "\n",
    "            y_pred = self.predict(x, weights, bias)\n",
    "            return -jnp.mean(y * jnp.log(y_pred) + (1 - y) * jnp.log(1 - y_pred))\n",
    "        return callable\n",
    "\n",
    "    def mean_squared_error(self):\n",
    "        @jax.jit\n",
    "        def callable(x, y, weights, bias):\n",
    "            \"\"\"\n",
    "                Computes the Mean Squared Error\n",
    "\n",
    "                Parameters:\n",
    "                x: ndarray - input of the regression model\n",
    "                y: ndarray - correct value of the output, one-hot representation\n",
    "                weights: ndarray - vector of weights\n",
    "                bias: float - bias term\n",
    "\n",
    "                Returns:\n",
    "                float - Mean Squared Error between the predictions of the regression model and the correct values\n",
    "\n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if weights and bias are not provided\n",
    "            \"\"\"\n",
    "\n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if weights is None or bias is None:\n",
    "                raise Exception(\"params (weights and/or bias) are not provided\")\n",
    "\n",
    "            y_pred = self.predict(x, weights, bias)\n",
    "            return jnp.mean((y_pred - y) ** 2)\n",
    "        return callable\n",
    "\n",
    "    # Metrics\n",
    "    def confusion_matrix(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the confusion matrix\n",
    "\n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "\n",
    "            Returns:\n",
    "            TP: float - true positives - attacks classified accurately as attacks\n",
    "            TN: float - true negatives - normal transactions accurately classified as normal\n",
    "            FP: float - false positives - normal traffic incorrectly classified as attacks\n",
    "            FN: float - false negatives - attacks incorrectly classified as normal\n",
    "        \"\"\"\n",
    "\n",
    "        TP = np.sum(np.logical_and(pred_labels == 1., true_labels == 1.))\n",
    "        TN = np.sum(np.logical_and(pred_labels == 0., true_labels == 0.))\n",
    "        FP = np.sum(np.logical_and(pred_labels == 1., true_labels == 0.))\n",
    "        FN = np.sum(np.logical_and(pred_labels == 0., true_labels == 1.))\n",
    "\n",
    "        return TP, TN, FP, FN\n",
    "\n",
    "    def accuracy(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the accuracy of the predictions\n",
    "\n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "\n",
    "            Returns:\n",
    "            float - accuracy of the artificial neural network, namely the number of samples\n",
    "                    correctly classified divided by the total number of samples\n",
    "        \"\"\"\n",
    "        TP, TN, _, _ = self.confusion_matrix(true_labels, pred_labels)\n",
    "        AC = ((TN + TP) / len(pred_labels)) * 100 # accuracy\n",
    "        return round(float(AC), 2)\n",
    "\n",
    "    def recall(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the recall (or sensitivity) of the predictions\n",
    "\n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "\n",
    "            Returns:\n",
    "            float - recall of the artificial neural network,\n",
    "                    namely the percentage of positive predictions (true positive rate),\n",
    "                    out of the total positive\n",
    "        \"\"\"\n",
    "        TP, _, _, FN = self.confusion_matrix(true_labels, pred_labels)\n",
    "        RC = (TP / (TP + FN)) * 100 # recall\n",
    "        return round(float(RC), 2)\n",
    "\n",
    "    def precision(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the precision of the predictions\n",
    "\n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "\n",
    "            Returns:\n",
    "            float - precision of the artificial neural network, namely the percentage of truly positive,\n",
    "                    out of all positive predicted\n",
    "        \"\"\"\n",
    "        TP, _, FP, _ = self.confusion_matrix(true_labels, pred_labels)\n",
    "        PR = (TP / (TP + FP)) * 100 # precision\n",
    "        return round(float(PR), 2)\n",
    "\n",
    "    def f1_score(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the F1 Score of the predictions\n",
    "\n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "\n",
    "            Returns:\n",
    "            float - f1 score of the artificial neural network, namely the harmonic mean of precision and recall.\n",
    "                    It takes both false positive and false negatives into account\n",
    "        \"\"\"\n",
    "        RC = self.recall(true_labels, pred_labels)\n",
    "        PR = self.precision(true_labels, pred_labels)\n",
    "        F1 = 2 * PR * RC / (PR + RC) # f1 score\n",
    "        return round(float(F1), 2)\n",
    "\n",
    "    def metrics(self, true_labels, pred_labels, metrics_df=None, dataset_label=''):\n",
    "        \"\"\"\n",
    "            Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
    "\n",
    "            Parameters:\n",
    "            predictions: ndarray - predictions of samples obtained with a model\n",
    "            true_labels: ndarray - true labels of the samples\n",
    "            metrics_df: DataFrame - DataFrame to which the computed statistics have to be put\n",
    "            dataset_label: str - label identifying the belonging of the statistics to its dataset\n",
    "\n",
    "            Returns:\n",
    "            DataFrame - DataFrame containing the statistics contained in the parameter metrics_df\n",
    "                        plus the statistics computed on the new predictions\n",
    "        \"\"\"\n",
    "\n",
    "        TP, TN, FP, FN = self.confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "        AC = self.accuracy(true_labels, pred_labels)\n",
    "        RC = self.recall(true_labels, pred_labels)\n",
    "        PR = self.precision(true_labels, pred_labels)\n",
    "        F1 = self.f1_score(true_labels, pred_labels)\n",
    "\n",
    "        if metrics_df is None:\n",
    "            columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
    "            metrics_df = pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)\n",
    "        else:\n",
    "            columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
    "            metrics_df = pd.concat([metrics_df, pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)], ignore_index=True)\n",
    "\n",
    "        return metrics_df\n",
    "\n",
    "    # Optimisation algorithms\n",
    "    def SGD(\n",
    "            self,\n",
    "            loss_function,\n",
    "            epochs=1000,\n",
    "            batch_size=128,\n",
    "            learning_rate_min=1e-3,\n",
    "            learning_rate_max=1e-1,\n",
    "            learning_rate_decay=1000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "           Trains the logistic regression classifier with Stochastic Gradient Descent method using mini-batches and\n",
    "            learning rate decay\n",
    "\n",
    "            Parameters:\n",
    "            loss_function: callable - loss function that it used in order to evaluate the cost\n",
    "                                        between the predictions and the correct values\n",
    "            epochs: int - number of epochs to perform\n",
    "            batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "            learning_rate_min: float - minimum learning rate used in the training phase\n",
    "            learning_rate_max: float - maximum learning rate used in the training phase\n",
    "            learning_rate_decay: float - learning rate decay used in the training phase\n",
    "\n",
    "            Returns:\n",
    "            weights: ndarray - optimized vector of weights\n",
    "            bias: float - optimized bias term\n",
    "            history: list - history of the loss function optimisation\n",
    "        \"\"\"\n",
    "        def callable(x_train, y_train, weights, bias):\n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "\n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=[2,3]))\n",
    "\n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, weights, bias))\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # Get learning rate\n",
    "                learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
    "\n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "\n",
    "                # Calculate gradient\n",
    "                grad_vals = grad_loss(x_train[idxs,:], y_train[idxs,:], weights, bias)\n",
    "\n",
    "                # Update weights and bias\n",
    "                weights = weights - learning_rate * grad_vals[0]\n",
    "                bias = bias - learning_rate * grad_vals[1]\n",
    "\n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, weights, bias))\n",
    "            return weights, bias, history\n",
    "        return callable\n",
    "\n",
    "    def RMSprop(\n",
    "            self,\n",
    "            loss_function,\n",
    "            epochs=1000,\n",
    "            batch_size=128,\n",
    "            learning_rate=0.1,\n",
    "            decay_rate = 0.9,\n",
    "            epsilon = 1e-8\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Trains the logistic regression classifier with Root Mean Square Propagation method\n",
    "\n",
    "            Parameters:\n",
    "            loss_function: callable - loss function that it used in order to evaluate the cost\n",
    "                                    between the predictions and the correct values\n",
    "            epochs: int - number of epochs to perform\n",
    "            batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "            learning_rate: float - learning rate used in the training phase\n",
    "            decay_rate: float - learning rate decay\n",
    "            epsilon: float - small constant to prevent division by zero (~1e-8)\n",
    "\n",
    "            Returns:\n",
    "            weights: ndarray - optimized vector of weights\n",
    "            bias: float - optimized bias term\n",
    "            history: list - history of the loss function optimisation\n",
    "        \"\"\"\n",
    "        def callable(x_train, y_train, weights, bias):\n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "\n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=[2,3]))\n",
    "\n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, weights, bias))\n",
    "\n",
    "            # Initialize cumulated square gradient\n",
    "            cumulated_square_grad = list()\n",
    "            cumulated_square_grad.append(np.zeros_like(weights))\n",
    "            cumulated_square_grad.append(np.zeros_like(bias))\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "\n",
    "                # Calculate gradient\n",
    "                grad_val = grad_loss(x_train[idxs,:], y_train[idxs,:], weights, bias)\n",
    "\n",
    "                # Update cumulated square gradient of weights\n",
    "                cumulated_square_grad[0] = decay_rate * cumulated_square_grad[0] + (1 - decay_rate) * grad_val[0] * grad_val[0]\n",
    "\n",
    "                # Update weights\n",
    "                weights = weights - learning_rate * grad_val[0] / (epsilon + np.sqrt(cumulated_square_grad[0]))\n",
    "\n",
    "                # Update cumulated square gradient of bias\n",
    "                cumulated_square_grad[1] = decay_rate * cumulated_square_grad[1] + (1 - decay_rate) * grad_val[1] * grad_val[1]\n",
    "\n",
    "                # Update bias\n",
    "                bias = bias - learning_rate * grad_val[1] / (epsilon + np.sqrt(cumulated_square_grad[1]))\n",
    "\n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, weights, bias))\n",
    "            return weights, bias, history\n",
    "        return callable\n",
    "\n",
    "    def train(self, x_train, y_train, weights, bias, optimizer):\n",
    "        \"\"\"\n",
    "            Trains the logistic regression using one the optimization algorithms\n",
    "\n",
    "            Parameters:\n",
    "            x_train: ndarray - training set of the dataset to fit\n",
    "            y_train: ndarray - training set's sample's labels\n",
    "            weights: ndarray - vector of weights\n",
    "            bias: float - bias term\n",
    "            optimizer: callable - optimization algorithm to be used in the training phase\n",
    "\n",
    "            Returns:\n",
    "            ndarray - updated weights and bias\n",
    "            ndarray - history of the loss function\n",
    "        \"\"\"\n",
    "\n",
    "        return optimizer(x_train, y_train, weights, bias)\n",
    "\n",
    "    def predict(self, x=None, weights=None, bias=None):\n",
    "        \"\"\"\n",
    "            Computes the predicted labels with logistic regression\n",
    "\n",
    "            Parameters:\n",
    "            x: ndarray - input of the logistic regression\n",
    "            weights: ndarray - vector of weights\n",
    "            bias: float - bias term\n",
    "\n",
    "            Returns:\n",
    "            ndarray - predicted values of the logistic regression\n",
    "\n",
    "            Raises:\n",
    "            Exception - if x is not provided\n",
    "            Exception - if params were not initialized\n",
    "        \"\"\"\n",
    "\n",
    "        if x is None:\n",
    "            raise Exception(\"x is not provided\")\n",
    "        if weights is None or bias is None:\n",
    "            raise Exception(\"Params (weights and/or bias) are not provided\")\n",
    "\n",
    "        # Algorithm\n",
    "        z = bias + x @ weights\n",
    "        y_pred = jax.nn.sigmoid(-z)\n",
    "\n",
    "        return y_pred"
   ],
   "id": "72300877ceefe08",
   "outputs": [],
   "execution_count": 456
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasets",
   "id": "c8afe9b7d7c070c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T14:56:02.626460Z",
     "start_time": "2025-02-02T14:56:02.558036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = list()\n",
    "\n",
    "v1 = ['V1', 'V5', 'V7', 'V8', 'V11', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'Amount', 'Class']\n",
    "datasets.append(data[v1])\n",
    "\n",
    "v2 = ['V1', 'V6', 'V13', 'V16', 'V17', 'V22', 'V23', 'V28', 'Amount', 'Class']\n",
    "datasets.append(data[v2])\n",
    "\n",
    "v3 = ['V2', 'V11', 'V12', 'V13', 'V15', 'V16', 'V17', 'V18', 'V20', 'V21', 'V24', 'V26', 'Amount', 'Class']\n",
    "datasets.append(data[v3])\n",
    "\n",
    "v4 = ['V2', 'V7', 'V10', 'V13', 'V15', 'V17', 'V19', 'V28', 'Amount', 'Class']\n",
    "datasets.append(data[v4])\n",
    "\n",
    "v5 = ['Time', 'V1', 'V7', 'V8', 'V9', 'V11', 'V12', 'V14', 'V15', 'V22', 'V27', 'V28', 'Amount', 'Class']\n",
    "datasets.append(data[v5])\n",
    "\n",
    "v6 = data.columns\n",
    "datasets.append(data[v6])\n",
    "\n",
    "v7 = ['V2', 'V4', 'V5', 'V6', 'V11', 'V12', 'V13', 'V16', 'V17', 'V18', 'V20', 'V21', 'V22', 'V23', 'V25', 'V26', 'V28', 'Amount', 'Class']\n",
    "datasets.append(data[v7])"
   ],
   "id": "9d15475a99802c17",
   "outputs": [],
   "execution_count": 457
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T14:56:03.969135Z",
     "start_time": "2025-02-02T14:56:03.965873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Logistic Regression classifier\n",
    "logistic_classifier = LogisticRegression()"
   ],
   "id": "f36cfb7d9e2d2219",
   "outputs": [],
   "execution_count": 458
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training and evaluation",
   "id": "1e3f726c97d7d153"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:26:37.048202Z",
     "start_time": "2025-02-02T18:18:25.158555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics_train_df = None\n",
    "metrics_test_df = None\n",
    "for i, dataset in enumerate(datasets):\n",
    "    # Get dataset (based on the feature vectors defined before)\n",
    "    input = dataset.to_numpy()\n",
    "\n",
    "    # Data splitting\n",
    "    x_train, y_train, _, _, x_test, y_test = data_split(data_input=input, train_size=0.8)\n",
    "\n",
    "    # SMOTE: oversampling\n",
    "    n_samples = 5000\n",
    "\n",
    "    x_minority = x_train[y_train[:, 0] == 1] # minority class samples (attacks)\n",
    "    x_train_synthetic = fit_resample(x_minority, n_samples=n_samples) # generate synthetic data for training\n",
    "    y_train_synthetic = np.ones((n_samples,1)) # generate other attack labels as well\n",
    "\n",
    "    # Add synthetic data to the original one\n",
    "    x_train_normalized = np.concatenate((x_train, x_train_synthetic), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_train_synthetic), axis=0)\n",
    "\n",
    "    # Training set normalisation\n",
    "    x_train_normalized, data_train_min, data_train_max = min_max(data=x_train_normalized)\n",
    "\n",
    "    # Validation set normalisation\n",
    "    # ...\n",
    "\n",
    "    # Testing set normalisation\n",
    "    x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    weights, bias = logistic_classifier.initialize_parameters(num_features=x_train_normalized.shape[1])\n",
    "\n",
    "    # Train logistic regression classifier\n",
    "    if i == 5:\n",
    "        updated_weights, updated_bias, history = logistic_classifier.train(\n",
    "            x_train = x_train_normalized,\n",
    "            y_train = y_train,\n",
    "            weights = weights,\n",
    "            bias = bias,\n",
    "            optimizer = logistic_classifier.RMSprop(\n",
    "                loss_function=logistic_classifier.cross_entropy(),\n",
    "                epochs=30000,\n",
    "                batch_size=50,\n",
    "                learning_rate=0.001,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        updated_weights, updated_bias, history = logistic_classifier.train(\n",
    "        x_train = x_train_normalized,\n",
    "        y_train = y_train,\n",
    "        weights = weights,\n",
    "        bias = bias,\n",
    "        optimizer = logistic_classifier.RMSprop(\n",
    "                loss_function=logistic_classifier.cross_entropy(),\n",
    "                epochs=30000,\n",
    "                batch_size=150,\n",
    "                learning_rate=0.001,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Get training predicted labels\n",
    "    train_pred_labels = logistic_classifier.predict(x_train_normalized, updated_weights, updated_bias)\n",
    "    train_pred_labels = train_pred_labels >= 0.5\n",
    "\n",
    "    # Get validation predicted labels\n",
    "    # ...\n",
    "\n",
    "    # Get testing predicted labels\n",
    "    test_pred_labels = logistic_classifier.predict(x_test_normalized, updated_weights, updated_bias)\n",
    "    test_pred_labels = test_pred_labels >= 0.5\n",
    "\n",
    "    # Print metrics\n",
    "    metrics_train_df = logistic_classifier.metrics(\n",
    "        true_labels=y_train,\n",
    "        pred_labels=train_pred_labels,\n",
    "        metrics_df=metrics_train_df,\n",
    "        dataset_label='v' + str(i+1) + ' training'\n",
    "    )\n",
    "    metrics_test_df = logistic_classifier.metrics(\n",
    "        true_labels=y_test,\n",
    "        pred_labels=test_pred_labels,\n",
    "        metrics_df=metrics_test_df,\n",
    "        dataset_label='v' + str(i+1) + ' testing'\n",
    "    )"
   ],
   "id": "dac804096cc879fd",
   "outputs": [],
   "execution_count": 487
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:32:06.137355Z",
     "start_time": "2025-02-02T18:32:06.119505Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_train_df",
   "id": "781c519ecfa44fcf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Set of features    TP      TN  FP    FN  accuracy  recall  precision  \\\n",
       "0     v1 training  3567  227387  67  1824     99.19   66.17      98.16   \n",
       "1     v2 training  1797  227427  27  3594     98.44   33.33      98.52   \n",
       "2     v3 training  3312  227407  47  2079     99.09   61.44      98.60   \n",
       "3     v4 training  1591  227427  27  3800     98.36   29.51      98.33   \n",
       "4     v5 training  3648  227389  65  1743     99.22   67.67      98.25   \n",
       "5     v6 training  3574  227412  42  1817     99.20   66.30      98.84   \n",
       "6     v7 training  3734  227397  57  1657     99.26   69.26      98.50   \n",
       "\n",
       "   F1-score  \n",
       "0     79.05  \n",
       "1     49.81  \n",
       "2     75.71  \n",
       "3     45.40  \n",
       "4     80.14  \n",
       "5     79.36  \n",
       "6     81.33  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set of features</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1 training</td>\n",
       "      <td>3567</td>\n",
       "      <td>227387</td>\n",
       "      <td>67</td>\n",
       "      <td>1824</td>\n",
       "      <td>99.19</td>\n",
       "      <td>66.17</td>\n",
       "      <td>98.16</td>\n",
       "      <td>79.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2 training</td>\n",
       "      <td>1797</td>\n",
       "      <td>227427</td>\n",
       "      <td>27</td>\n",
       "      <td>3594</td>\n",
       "      <td>98.44</td>\n",
       "      <td>33.33</td>\n",
       "      <td>98.52</td>\n",
       "      <td>49.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v3 training</td>\n",
       "      <td>3312</td>\n",
       "      <td>227407</td>\n",
       "      <td>47</td>\n",
       "      <td>2079</td>\n",
       "      <td>99.09</td>\n",
       "      <td>61.44</td>\n",
       "      <td>98.60</td>\n",
       "      <td>75.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v4 training</td>\n",
       "      <td>1591</td>\n",
       "      <td>227427</td>\n",
       "      <td>27</td>\n",
       "      <td>3800</td>\n",
       "      <td>98.36</td>\n",
       "      <td>29.51</td>\n",
       "      <td>98.33</td>\n",
       "      <td>45.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v5 training</td>\n",
       "      <td>3648</td>\n",
       "      <td>227389</td>\n",
       "      <td>65</td>\n",
       "      <td>1743</td>\n",
       "      <td>99.22</td>\n",
       "      <td>67.67</td>\n",
       "      <td>98.25</td>\n",
       "      <td>80.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>v6 training</td>\n",
       "      <td>3574</td>\n",
       "      <td>227412</td>\n",
       "      <td>42</td>\n",
       "      <td>1817</td>\n",
       "      <td>99.20</td>\n",
       "      <td>66.30</td>\n",
       "      <td>98.84</td>\n",
       "      <td>79.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v7 training</td>\n",
       "      <td>3734</td>\n",
       "      <td>227397</td>\n",
       "      <td>57</td>\n",
       "      <td>1657</td>\n",
       "      <td>99.26</td>\n",
       "      <td>69.26</td>\n",
       "      <td>98.50</td>\n",
       "      <td>81.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 488
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:32:08.565379Z",
     "start_time": "2025-02-02T18:32:08.558503Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_test_df",
   "id": "dd57cf8b5d66cfcb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Set of features  TP     TN  FP  FN  accuracy  recall  precision  F1-score\n",
       "0      v1 testing  60  56849  12  41     99.91   59.41      83.33     69.37\n",
       "1      v2 testing  31  56859   2  70     99.87   30.69      93.94     46.27\n",
       "2      v3 testing  51  56855   6  50     99.90   50.50      89.47     64.56\n",
       "3      v4 testing  27  56859   2  74     99.87   26.73      93.10     41.53\n",
       "4      v5 testing  65  56847  14  36     99.91   64.36      82.28     72.23\n",
       "5      v6 testing  62  56854   7  39     99.92   61.39      89.86     72.95\n",
       "6      v7 testing  62  56852   9  39     99.92   61.39      87.32     72.09"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set of features</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1 testing</td>\n",
       "      <td>60</td>\n",
       "      <td>56849</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>99.91</td>\n",
       "      <td>59.41</td>\n",
       "      <td>83.33</td>\n",
       "      <td>69.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2 testing</td>\n",
       "      <td>31</td>\n",
       "      <td>56859</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>99.87</td>\n",
       "      <td>30.69</td>\n",
       "      <td>93.94</td>\n",
       "      <td>46.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v3 testing</td>\n",
       "      <td>51</td>\n",
       "      <td>56855</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>99.90</td>\n",
       "      <td>50.50</td>\n",
       "      <td>89.47</td>\n",
       "      <td>64.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v4 testing</td>\n",
       "      <td>27</td>\n",
       "      <td>56859</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>99.87</td>\n",
       "      <td>26.73</td>\n",
       "      <td>93.10</td>\n",
       "      <td>41.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v5 testing</td>\n",
       "      <td>65</td>\n",
       "      <td>56847</td>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>99.91</td>\n",
       "      <td>64.36</td>\n",
       "      <td>82.28</td>\n",
       "      <td>72.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>v6 testing</td>\n",
       "      <td>62</td>\n",
       "      <td>56854</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>99.92</td>\n",
       "      <td>61.39</td>\n",
       "      <td>89.86</td>\n",
       "      <td>72.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v7 testing</td>\n",
       "      <td>62</td>\n",
       "      <td>56852</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>99.92</td>\n",
       "      <td>61.39</td>\n",
       "      <td>87.32</td>\n",
       "      <td>72.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 489
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
