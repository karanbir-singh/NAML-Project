{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:16:27.176262Z",
     "start_time": "2025-01-27T17:16:27.167607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.oversampling import fit_resample\n",
    "from utils.data_processer import *"
   ],
   "id": "57daa5998f08eb3a",
   "outputs": [],
   "execution_count": 298
  },
  {
   "cell_type": "code",
   "id": "bd778b3540b7a063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:16:29.508316Z",
     "start_time": "2025-01-27T17:16:28.324790Z"
    }
   },
   "source": [
    "data = pd.read_csv(\"../creditcard_2021.csv\")\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "print(f\"Number of fraudolent transaction: {(data['Class'] == 1).sum()}\")\n",
    "print(f\"Ratio of fraudolent transaction: {data['Class'].mean()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 284807\n",
      "Number of fraudolent transaction: 492\n",
      "Ratio of fraudolent transaction: 0.001727485630620034\n"
     ]
    }
   ],
   "execution_count": 299
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Artificial Neural Network\n",
    "## Definition"
   ],
   "id": "c9d3f7f7f841843b"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-27T17:21:12.861134Z",
     "start_time": "2025-01-27T17:21:12.790901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ANN:\n",
    "    def __init__(self, layers_size=None, act_func=jnp.tanh, out_act_fun = jax.nn.sigmoid):\n",
    "        self.layers_size = layers_size\n",
    "        self.act_func = act_func \n",
    "        self.out_act_fun = out_act_fun\n",
    "\n",
    "    def initialize_parameters(self, layers_size=None):\n",
    "        \"\"\"\n",
    "            Sets the parameters of the artificial neural network given the number of\n",
    "            neurons of its layers, namely it sets the matrix of weights and the bias\n",
    "            vector for each layer \n",
    "          \n",
    "            Parameters:\n",
    "            layers_size: list - ordered sizes of the layers of the artificial neural network\n",
    "            \n",
    "            Raises:\n",
    "            exception: if layers_size is not provided\n",
    "        \"\"\"\n",
    "\n",
    "        if layers_size is None:\n",
    "            raise Exception(\"Size of layers not provided\")\n",
    "        \n",
    "        layers_size = jnp.array(layers_size)\n",
    "        \n",
    "        np.random.seed(0) # For reproducibility\n",
    "        self.layers_size = layers_size\n",
    "        params = list()\n",
    "        \n",
    "        for i in range(len(self.layers_size) - 1):\n",
    "            W = np.random.randn(self.layers_size[i+1], self.layers_size[i])\n",
    "            b = np.zeros((self.layers_size[i+1], 1))\n",
    "            params.append(W)\n",
    "            params.append(b)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def MSW(self, params):\n",
    "        \"\"\"\n",
    "        Computes the sum of the squared values of the weights of the artificial neural\n",
    "        network\n",
    "    \n",
    "        Parameters:\n",
    "        params: list - parameters of the artificial neural network, namely weights and biases\n",
    "        \n",
    "        Returns:\n",
    "        float - sum of the squared values of the weights of the artificial neural network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract weights\n",
    "        weights = params[::2]\n",
    "        \n",
    "        # Calculate MSW\n",
    "        partial_sum = 0.0\n",
    "        n_weights = 0\n",
    "        for W in weights:\n",
    "            partial_sum = partial_sum + jnp.sum(W * W)\n",
    "            n_weights = n_weights + W.size\n",
    "            \n",
    "        return partial_sum / n_weights\n",
    "    \n",
    "    # Metrics\n",
    "    def confusion_matrix(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the confusion matrix\n",
    "        \n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "    \n",
    "            Returns:\n",
    "            TP: float - true positives - attacks classified accurately as attacks\n",
    "            TN: float - true negatives - normal transactions accurately classified as normal\n",
    "            FP: float - false positives - normal traffic incorrectly classified as attacks\n",
    "            FN: float - false negatives - attacks incorrectly classified as normal\n",
    "        \"\"\"\n",
    "    \n",
    "        TP = np.sum(np.logical_and(pred_labels == 1., true_labels == 1.))\n",
    "        TN = np.sum(np.logical_and(pred_labels == 0., true_labels == 0.)) \n",
    "        FP = np.sum(np.logical_and(pred_labels == 1., true_labels == 0.))\n",
    "        FN = np.sum(np.logical_and(pred_labels == 0., true_labels == 1.))\n",
    "        \n",
    "        return TP, TN, FP, FN\n",
    "    \n",
    "    def accuracy(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the accuracy of the predictions\n",
    "          \n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "          \n",
    "            Returns:\n",
    "            float - accuracy of the artificial neural network, namely the number of samples\n",
    "                    correctly classified divided by the total number of samples\n",
    "        \"\"\"\n",
    "        TP, TN, _, _ = self.confusion_matrix(true_labels, pred_labels)\n",
    "        AC = ((TN + TP) / len(pred_labels)) * 100 # accuracy\n",
    "        return round(float(AC), 2)\n",
    "    \n",
    "    def recall(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the recall (or sensitivity) of the predictions\n",
    "          \n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "          \n",
    "            Returns:\n",
    "            float - recall of the artificial neural network, \n",
    "                    namely the percentage of positive predictions (true positive rate),\n",
    "                    out of the total positive\n",
    "        \"\"\"\n",
    "        TP, _, _, FN = self.confusion_matrix(true_labels, pred_labels)\n",
    "        RC = (TP / (TP + FN)) * 100 # recall\n",
    "        return round(float(RC), 2)\n",
    "    \n",
    "    def precision(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the precision of the predictions\n",
    "          \n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "          \n",
    "            Returns:\n",
    "            float - precision of the artificial neural network, namely the percentage of truly positive,\n",
    "                    out of all positive predicted    \n",
    "        \"\"\"\n",
    "        TP, _, FP, _ = self.confusion_matrix(true_labels, pred_labels)\n",
    "        PR = (TP / (TP + FP)) * 100 # precision\n",
    "        return round(float(PR), 2)\n",
    "    \n",
    "    def f1_score(self, true_labels, pred_labels):\n",
    "        \"\"\"\n",
    "            Computes the F1 Score of the predictions\n",
    "          \n",
    "            Parameters:\n",
    "            true_labels: ndarray - correct values of the samples' class'\n",
    "            pred_labels: ndarray - predicted values of the samples' class'\n",
    "          \n",
    "            Returns:\n",
    "            float - f1 score of the artificial neural network, namely the harmonic mean of precision and recall. \n",
    "                    It takes both false positive and false negatives into account\n",
    "        \"\"\"\n",
    "        RC = self.recall(true_labels, pred_labels)\n",
    "        PR = self.precision(true_labels, pred_labels)\n",
    "        F1 = 2 * PR * RC / (PR + RC) # f1 score\n",
    "        return round(float(F1), 2)\n",
    "    \n",
    "    def metrics(self, true_labels, pred_labels, metrics_df=None, dataset_label=''):\n",
    "        \"\"\"\n",
    "            Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
    "        \n",
    "            Parameters:\n",
    "            predictions: ndarray - predictions of samples obtained with a model\n",
    "            true_labels: ndarray - true labels of the samples\n",
    "            metrics_df: DataFrame - DataFrame to which the computed statistics have to be put\n",
    "            dataset_label: str - label identifying the belonging of the statistics to its dataset\n",
    "        \n",
    "            Returns:\n",
    "            DataFrame - DataFrame containing the statistics contained in the parameter metrics_df\n",
    "                        plus the statistics computed on the new predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        TP, TN, FP, FN = self.confusion_matrix(true_labels, pred_labels) \n",
    "        \n",
    "        AC = self.accuracy(true_labels, pred_labels)\n",
    "        RC = self.recall(true_labels, pred_labels)\n",
    "        PR = self.precision(true_labels, pred_labels)\n",
    "        F1 = self.f1_score(true_labels, pred_labels)\n",
    "        \n",
    "        if metrics_df is None:\n",
    "            columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
    "            metrics_df = pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)\n",
    "        else:\n",
    "            columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
    "            metrics_df = pd.concat([metrics_df, pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)], ignore_index=True)\n",
    "          \n",
    "        return metrics_df\n",
    "    \n",
    "    # Loss functions\n",
    "    def cross_entropy(self):\n",
    "        @jax.jit\n",
    "        def callable(x=None, y=None, params=None):\n",
    "            \"\"\"\n",
    "                Computes the Cross Entropy Cost function\n",
    "                \n",
    "                Parameters:\n",
    "                x: ndarray - input of the artificial neural network\n",
    "                y: ndarray - correct value of the output, one-hot representation\n",
    "                \n",
    "                Returns:\n",
    "                float - Cross Entropy Cost between the predictions of the artificial neural network and the correct values\n",
    "                \n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if params are not provided\n",
    "            \"\"\"\n",
    "            \n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if params is None:\n",
    "                raise Exception(\"params are not provided\")\n",
    "        \n",
    "            y_pred = self.predict(x, params)\n",
    "            return -jnp.mean(y * jnp.log(y_pred) + (1 - y) * jnp.log(1 - y_pred))\n",
    "        return callable\n",
    "    \n",
    "    def mean_squared_error(self):\n",
    "        @jax.jit\n",
    "        def callable(x, y, params):\n",
    "            \"\"\"\n",
    "                Computes the Mean Squared Error\n",
    "                \n",
    "                Parameters:\n",
    "                x: ndarray - input of the artificial neural network\n",
    "                y: ndarray - correct value of the output, one-hot representation\n",
    "                \n",
    "                Returns:\n",
    "                float - Mean Squared Error between the predictions of the artificial neural network and the correct values\n",
    "                \n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if params are not provided\n",
    "            \"\"\"\n",
    "            \n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if params is None:\n",
    "                raise Exception(\"params are not provided\")\n",
    "            \n",
    "            y_pred = self.predict(x, params)\n",
    "            return jnp.mean((y_pred - y) ** 2)\n",
    "        return callable\n",
    "    \n",
    "    def regularized_loss(self, loss_function, penalization):\n",
    "        @jax.jit\n",
    "        def callable(x=None, y=None, params=None):\n",
    "            \"\"\"\n",
    "                Computes the loss function applying regularization to the given loss function with penalization \n",
    "            \n",
    "                Parameters:\n",
    "                x: ndarray - input of the artificial neural network\n",
    "                y: ndarray - correct value of the output\n",
    "                params: list - parameters of the artificial neural network, namely weights and biases\n",
    "                penalization: float - weight to which the MSW is multiplied and \n",
    "                                    that makes possible to modify the impact of the regularization term\n",
    "                Returns:\n",
    "                float - loss function value between the predictions of the artificial neural network\n",
    "                        and the correct values with regularization term\n",
    "              \n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if params are not provided\n",
    "            \"\"\"\n",
    "                \n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if params is None:\n",
    "                raise Exception(\"params are not provided\")\n",
    "            \n",
    "            return loss_function(x, y, params) + penalization/(2 * x.shape[0]) * self.MSW(params)\n",
    "        return callable\n",
    "    \n",
    "    # Optimisation algorithms\n",
    "    def SGD(\n",
    "            self, \n",
    "            loss_function, \n",
    "            epochs=1000, \n",
    "            batch_size=128, \n",
    "            learning_rate_min=1e-3, \n",
    "            learning_rate_max=1e-1, \n",
    "            learning_rate_decay=1000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "           Trains the artificial neural network with Stochastic Gradient Descent method using mini-batches and\n",
    "           learning rate decay\n",
    "        \n",
    "            Parameters:\n",
    "            loss_function: callable - loss function that it used in order to evaluate the cost \n",
    "                                        between the predictions and the correct values\n",
    "            epochs: int - number of epochs to perform\n",
    "            batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "            learning_rate_min: float - minimum learning rate used in the training phase\n",
    "            learning_rate_max: float - maximum learning rate used in the training phase\n",
    "            learning_rate_decay: float - learning rate decay used in the training phase\n",
    "        \n",
    "            Returns:\n",
    "            params: list - trained parameters of the artificial neural network, \n",
    "                            namely weights and biases optimized for fitting the training set\n",
    "            history: list - history of the loss function optimisation\n",
    "        \"\"\"\n",
    "        def callable(x_train, y_train, params):\n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "        \n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=2))\n",
    "        \n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, params))\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Get learning rate\n",
    "                learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
    "        \n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "                \n",
    "                # Calculate gradient\n",
    "                grad_val = grad_loss(x_train[idxs,:], y_train[idxs,:], params)\n",
    "        \n",
    "                # Update params\n",
    "                for i in range(len(params)):\n",
    "                    params[i] = params[i] - learning_rate * grad_val[i]\n",
    "                \n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, params))\n",
    "            return params, history\n",
    "        return callable\n",
    "    \n",
    "    def SGD_momentum(\n",
    "            self, \n",
    "            loss_function, \n",
    "            epochs=1000, \n",
    "            batch_size=128, \n",
    "            learning_rate_min=1e-3, \n",
    "            learning_rate_max=1e-1, \n",
    "            learning_rate_decay=1000,\n",
    "            momentum=0.9,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Trains the artificial neural network with Stochastic Gradient Descent method with Momentum \n",
    "            using mini-batches and learning rate decay\n",
    "        \n",
    "            Parameters:\n",
    "            loss_function: callable - loss function that it used in order to evaluate the cost \n",
    "                                    between the predictions and the correct values\n",
    "            epochs: int - number of epochs to perform\n",
    "            batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "            learning_rate_min: float - minimum learning rate used in the training phase\n",
    "            learning_rate_max: float - maximum learning rate used in the training phase\n",
    "            learning_rate_decay: float - learning rate decay used in the training phase\n",
    "            momentum: float - momentum used in the training phase\n",
    "        \n",
    "            Returns:\n",
    "            params: list - trained parameters of the artificial neural network, \n",
    "                            namely weights and biases optimized for fitting the training set\n",
    "            history: list - history of the loss function optimisation\n",
    "        \"\"\"\n",
    "        def callable(x_train, y_train, params):\n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "        \n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=2))\n",
    "            \n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, params))\n",
    "            \n",
    "            # Initialize velocity\n",
    "            velocity = list()\n",
    "            for i in range(len(params)):\n",
    "                velocity.append(np.zeros_like(params[i]))\n",
    "                \n",
    "            for epoch in range(epochs):\n",
    "                # Get learning rate\n",
    "                learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
    "        \n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "                \n",
    "                # Calculate gradient\n",
    "                grad_val = grad_loss(x_train[idxs,:], y_train[idxs,:], params)\n",
    "                    \n",
    "                for i in range(len(params)):\n",
    "                    # Compute velocity[i]\n",
    "                    velocity[i] = momentum * velocity[i] - learning_rate * grad_val[i]\n",
    "\n",
    "                    # Update params[i]\n",
    "                    params[i] = params[i] + velocity[i]\n",
    "                \n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, params))\n",
    "            return params, history\n",
    "        return callable\n",
    "    \n",
    "    def NAG(\n",
    "            self, \n",
    "            loss_function, \n",
    "            epochs=1000, \n",
    "            batch_size=128, \n",
    "            learning_rate_min=1e-3, \n",
    "            learning_rate_max=1e-1, \n",
    "            learning_rate_decay=1000,\n",
    "            momentum=0.9,\n",
    "    ):\n",
    "        def callable(x_train, y_train, params):\n",
    "            \"\"\"\n",
    "                Trains the artificial neural network with Nesterov Accelerated Gradient method\n",
    "            \n",
    "                Parameters:\n",
    "                loss_function: callable - loss function that it used in order to evaluate the cost \n",
    "                                        between the predictions and the correct values\n",
    "                epochs: int - number of epochs to perform\n",
    "                batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "                learning_rate_min: float - minimum learning rate used in the training phase\n",
    "                learning_rate_max: float - maximum learning rate used in the training phase\n",
    "                learning_rate_decay: float - learning rate decay used in the training phase\n",
    "                momentum: float - momentum used in the training phase\n",
    "            \n",
    "                Returns:\n",
    "                params: list - trained parameters of the artificial neural network, \n",
    "                            namely weights and biases optimized for fitting the training set\n",
    "                history: list - history of the loss function optimisation\n",
    "            \"\"\"\n",
    "    \n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "        \n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=2))\n",
    "            \n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, params))\n",
    "            \n",
    "            # Initialize velocity\n",
    "            velocity = list()\n",
    "            for i in range(len(params)):\n",
    "                velocity.append(np.zeros_like(params[i]))\n",
    "                \n",
    "            for epoch in range(epochs):\n",
    "                # Get learning rate\n",
    "                learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
    "        \n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "                \n",
    "                \n",
    "                # Calculate gradient: \n",
    "                # here it's necessary to calculate the arguments that will substitute 'params' on gradient evaluation\n",
    "                grad_args = list()\n",
    "                for i in range(len(params)):\n",
    "                    grad_args.append(np.zeros_like(params[i]))\n",
    "                for i in range(len(params)):\n",
    "                    grad_args[i] = params[i] - momentum * velocity[i]\n",
    "                \n",
    "                grad_val = grad_loss(x_train[idxs,:], y_train[idxs,:], grad_args)\n",
    "                    \n",
    "                for i in range(len(params)):\n",
    "                    # Compute velocity[i]\n",
    "                    velocity[i] = momentum * velocity[i] + learning_rate * grad_val[i]\n",
    "\n",
    "                    # Update params[i]\n",
    "                    params[i] = params[i] - velocity[i]\n",
    "                \n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, params))\n",
    "            return params, history\n",
    "        return callable\n",
    "    \n",
    "    def RMSprop(\n",
    "            self, \n",
    "            loss_function, \n",
    "            epochs=1000, \n",
    "            batch_size=128, \n",
    "            learning_rate=0.1,\n",
    "            decay_rate = 0.9,\n",
    "            epsilon = 1e-8\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Trains the artificial neural network with Root Mean Square Propagation method\n",
    "        \n",
    "            Parameters:\n",
    "            loss_function: callable - loss function that it used in order to evaluate the cost \n",
    "                                    between the predictions and the correct values\n",
    "            epochs: int - number of epochs to perform\n",
    "            batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "            learning_rate: float - learning rate used in the training phase\n",
    "            decay_rate: float - learning rate decay\n",
    "            epsilon: float - small constant to prevent division by zero (~1e-8)\n",
    "        \n",
    "            Returns:\n",
    "            params: list - trained parameters of the artificial neural network, \n",
    "                            namely weights and biases optimized for fitting the training set\n",
    "            history: list - history of the loss function optimisation\n",
    "        \"\"\"\n",
    "        def callable(x_train, y_train, params):\n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "        \n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=2))\n",
    "            \n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, params))\n",
    "            \n",
    "            # Initialize cumulated square gradient\n",
    "            cumulated_square_grad = list()\n",
    "            for i in range(len(params)):\n",
    "                cumulated_square_grad.append(np.zeros_like(params[i]))\n",
    "                \n",
    "            for epoch in range(epochs):        \n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "                \n",
    "                # Calculate gradient\n",
    "                grad_val = grad_loss(x_train[idxs,:], y_train[idxs,:], params)\n",
    "                    \n",
    "                for i in range(len(params)):\n",
    "                    # Update cumulated square gradient\n",
    "                    cumulated_square_grad[i] = decay_rate * cumulated_square_grad[i] + (1 - decay_rate) * grad_val[i] * grad_val[i]\n",
    "            \n",
    "                    # Update params[i]\n",
    "                    params[i] = params[i] - learning_rate * grad_val[i] / (epsilon + np.sqrt(cumulated_square_grad[i]))\n",
    "                \n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, params))\n",
    "            return params, history\n",
    "        return callable\n",
    "    \n",
    "    def train(self, x_train, y_train, params, optimizer):\n",
    "        \"\"\"\n",
    "            Trains the artificial neural network using one the optimization algorithms\n",
    "        \n",
    "            Parameters:\n",
    "            x_train: ndarray - training set of the dataset to fit\n",
    "            y_train: ndarray - training set's sample's labels\n",
    "            params: ndarray - parameters of the artificial neural network, namely weights and biases\n",
    "            optimizer: callable - optimization algorithm to be used in the training phase\n",
    "\n",
    "            Returns:\n",
    "            ndarray - updated weights and bias\n",
    "            ndarray - history of the loss function\n",
    "        \"\"\"\n",
    "        \n",
    "        return optimizer(x_train, y_train, params)\n",
    "    \n",
    "    def predict(self, x=None, params=None):\n",
    "        \"\"\"\n",
    "            Computes the value of the output of the artificial neural network given an input\n",
    "        \n",
    "            Parameters:\n",
    "            x: ndarray - input of the artificial neural network\n",
    "            act_func: callable - activation function of the artificial neural network\n",
    "            out_act_fun: callable - output activation function of the artificial neural network\n",
    "        \n",
    "            Returns: \n",
    "            ndarray - output value of the artificial neural network\n",
    "                \n",
    "            Raises:\n",
    "            Exception - if x is not provided\n",
    "            Exception - if parameters were not initialized\n",
    "        \"\"\"\n",
    "        \n",
    "        if x is None:\n",
    "            raise Exception(\"x is not provided\")\n",
    "        if params is None:\n",
    "            raise Exception(\"Parameters were not initialized\")\n",
    "        \n",
    "        # Number of ANN layers\n",
    "        num_layers = int(len(self.layers_size)) + 1\n",
    "        \n",
    "        # Algorithm\n",
    "        layer = x.T\n",
    "        weights = params[0::2]\n",
    "        biases = params[1::2]\n",
    "        for i in range(num_layers - 2):\n",
    "            # Update layer values\n",
    "            layer = weights[i] @ layer + biases[i]\n",
    "            \n",
    "            # Apply activation function\n",
    "            layer = self.act_func(layer)\n",
    "                  \n",
    "        # On the output layer it is applied the sigmoid function \n",
    "        # since the output is needed to be between 0 and 1\n",
    "        layer = self.out_act_fun(layer)\n",
    "        layer = layer.T\n",
    "        \n",
    "        return layer"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 305
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasets",
   "id": "cb777bef8f1b78a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:16:34.347671Z",
     "start_time": "2025-01-27T17:16:34.278042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = list()\n",
    "\n",
    "v1 = ['V1', 'V5', 'V7', 'V8', 'V11', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'Amount', 'Class']\n",
    "datasets.append(data[v1])\n",
    "\n",
    "v2 = ['V1', 'V6', 'V13', 'V16', 'V17', 'V22', 'V23', 'V28', 'Amount', 'Class']\n",
    "datasets.append(data[v2])\n",
    "\n",
    "v3 = ['V2', 'V11', 'V12', 'V13', 'V15', 'V16', 'V17', 'V18', 'V20', 'V21', 'V24', 'V26', 'Amount', 'Class']\n",
    "datasets.append(data[v3])\n",
    "\n",
    "v4 = ['V2', 'V7', 'V10', 'V13', 'V15', 'V17', 'V19', 'V28', 'Amount', 'Class']\n",
    "datasets.append(data[v4])\n",
    "\n",
    "v5 = ['Time', 'V1', 'V7', 'V8', 'V9', 'V11', 'V12', 'V14', 'V15', 'V22', 'V27', 'V28', 'Amount', 'Class']\n",
    "datasets.append(data[v5])\n",
    "\n",
    "# Dataset with full feature vector\n",
    "v6 = data.columns\n",
    "datasets.append(data[v6])"
   ],
   "id": "eb28e7c674d5cba6",
   "outputs": [],
   "execution_count": 301
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:21:16.091455Z",
     "start_time": "2025-01-27T17:21:16.087499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ANN with:\n",
    "network = ANN(act_func=jnp.tanh, out_act_fun=jax.nn.sigmoid)"
   ],
   "id": "473f8995aee30b86",
   "outputs": [],
   "execution_count": 306
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training and evaluation",
   "id": "a01a9a9846250598"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_train_df = None\n",
    "metrics_test_df = None\n",
    "for i, dataset in enumerate(datasets):\n",
    "    # Get dataset (based on the feature vectors defined before)\n",
    "    input = dataset.to_numpy()\n",
    "\n",
    "    # Data splitting\n",
    "    x_train, y_train, _, _, x_test, y_test = data_split(data_input=input, train_size=0.8)\n",
    "    \n",
    "    # SMOTE: oversampling\n",
    "    n_samples = 7500\n",
    "    x_minority = x_train[y_train[:, 0] == 1] # minority class samples (attacks)\n",
    "    x_train_synthetic = fit_resample(x_minority, n_samples=n_samples) # generate synthetic data for training\n",
    "    y_train_synthetic = np.ones((n_samples,1)) # generate other attack labels as well\n",
    "    \n",
    "    # Add synthetic data to the original one\n",
    "    x_train_normalized = np.concatenate((x_train, x_train_synthetic), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_train_synthetic), axis=0)\n",
    "    \n",
    "    # Training set normalisation\n",
    "    x_train_normalized, data_train_min, data_train_max = min_max(data=x_train_normalized)\n",
    "    \n",
    "    # Validation set normalisation\n",
    "    # ...\n",
    "    \n",
    "    # Testing set normalisation\n",
    "    x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    params = network.initialize_parameters([x_train_normalized.shape[1], 30, 20, 1])\n",
    "    \n",
    "    # Train ann, with gradient descent - mean squared error\n",
    "    updated_params, history = network.train(\n",
    "        x_train = x_train_normalized, \n",
    "        y_train = y_train, \n",
    "        params = params,    \n",
    "        optimizer = network.RMSprop(\n",
    "            loss_function=network.regularized_loss(network.cross_entropy(), penalization=0.5),\n",
    "            epochs=2000,\n",
    "            batch_size=256,\n",
    "            learning_rate=0.001,\n",
    "            decay_rate=0.8,\n",
    "            epsilon=1e-7,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Get training predicted labels\n",
    "    train_pred_labels = network.predict(x_train_normalized, updated_params)\n",
    "    train_pred_labels = train_pred_labels >= 0.5\n",
    "    \n",
    "    # Get validation predicted labels\n",
    "    # ...\n",
    "    \n",
    "    # Get testing predicted labels\n",
    "    test_pred_labels = network.predict(x_test_normalized, updated_params)\n",
    "    test_pred_labels = test_pred_labels >= 0.5\n",
    "    \n",
    "    # Print metrics\n",
    "    metrics_train_df = network.metrics(\n",
    "        true_labels=y_train, \n",
    "        pred_labels=train_pred_labels, \n",
    "        metrics_df=metrics_train_df,\n",
    "        dataset_label='v' + str(i+1) + ' training'\n",
    "    )\n",
    "    metrics_test_df = network.metrics(\n",
    "        true_labels=y_test, \n",
    "        pred_labels=test_pred_labels, \n",
    "        metrics_df=metrics_test_df,\n",
    "        dataset_label='v' + str(i+1) + ' testing'\n",
    "    )"
   ],
   "id": "78ad6fb8708346af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:24:36.541643Z",
     "start_time": "2025-01-27T17:24:36.492361Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_train_df",
   "id": "a8f9f4a0697c19f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Set of features    TP      TN   FP    FN  accuracy  recall  precision  \\\n",
       "0     v1 training  6186  227290  164  1705     99.21   78.39      97.42   \n",
       "1     v2 training  5444  227386   68  2447     98.93   68.99      98.77   \n",
       "2     v3 training  6070  227359   95  1821     99.19   76.92      98.46   \n",
       "3     v4 training  5957  227377   77  1934     99.15   75.49      98.72   \n",
       "4     v5 training  6868  227132  322  1023     99.43   87.04      95.52   \n",
       "5     v6 training  6874  227371   83  1017     99.53   87.11      98.81   \n",
       "\n",
       "   F1-score  \n",
       "0     86.88  \n",
       "1     81.24  \n",
       "2     86.37  \n",
       "3     85.56  \n",
       "4     91.08  \n",
       "5     92.59  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set of features</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1 training</td>\n",
       "      <td>6186</td>\n",
       "      <td>227290</td>\n",
       "      <td>164</td>\n",
       "      <td>1705</td>\n",
       "      <td>99.21</td>\n",
       "      <td>78.39</td>\n",
       "      <td>97.42</td>\n",
       "      <td>86.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2 training</td>\n",
       "      <td>5444</td>\n",
       "      <td>227386</td>\n",
       "      <td>68</td>\n",
       "      <td>2447</td>\n",
       "      <td>98.93</td>\n",
       "      <td>68.99</td>\n",
       "      <td>98.77</td>\n",
       "      <td>81.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v3 training</td>\n",
       "      <td>6070</td>\n",
       "      <td>227359</td>\n",
       "      <td>95</td>\n",
       "      <td>1821</td>\n",
       "      <td>99.19</td>\n",
       "      <td>76.92</td>\n",
       "      <td>98.46</td>\n",
       "      <td>86.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v4 training</td>\n",
       "      <td>5957</td>\n",
       "      <td>227377</td>\n",
       "      <td>77</td>\n",
       "      <td>1934</td>\n",
       "      <td>99.15</td>\n",
       "      <td>75.49</td>\n",
       "      <td>98.72</td>\n",
       "      <td>85.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v5 training</td>\n",
       "      <td>6868</td>\n",
       "      <td>227132</td>\n",
       "      <td>322</td>\n",
       "      <td>1023</td>\n",
       "      <td>99.43</td>\n",
       "      <td>87.04</td>\n",
       "      <td>95.52</td>\n",
       "      <td>91.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>v6 training</td>\n",
       "      <td>6874</td>\n",
       "      <td>227371</td>\n",
       "      <td>83</td>\n",
       "      <td>1017</td>\n",
       "      <td>99.53</td>\n",
       "      <td>87.11</td>\n",
       "      <td>98.81</td>\n",
       "      <td>92.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 308
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:24:53.550398Z",
     "start_time": "2025-01-27T17:24:53.531675Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_test_df",
   "id": "4dc6da1194485c9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Set of features  TP     TN  FP  FN  accuracy  recall  precision  F1-score\n",
       "0      v1 testing  79  56820  41  22     99.89   78.22      65.83     71.49\n",
       "1      v2 testing  67  56843  18  34     99.91   66.34      78.82     72.04\n",
       "2      v3 testing  78  56841  20  23     99.92   77.23      79.59     78.39\n",
       "3      v4 testing  76  56840  21  25     99.92   75.25      78.35     76.77\n",
       "4      v5 testing  88  56772  89  13     99.82   87.13      49.72     63.31\n",
       "5      v6 testing  82  56841  20  19     99.93   81.19      80.39     80.79"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set of features</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1 testing</td>\n",
       "      <td>79</td>\n",
       "      <td>56820</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>99.89</td>\n",
       "      <td>78.22</td>\n",
       "      <td>65.83</td>\n",
       "      <td>71.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2 testing</td>\n",
       "      <td>67</td>\n",
       "      <td>56843</td>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>99.91</td>\n",
       "      <td>66.34</td>\n",
       "      <td>78.82</td>\n",
       "      <td>72.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v3 testing</td>\n",
       "      <td>78</td>\n",
       "      <td>56841</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>99.92</td>\n",
       "      <td>77.23</td>\n",
       "      <td>79.59</td>\n",
       "      <td>78.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v4 testing</td>\n",
       "      <td>76</td>\n",
       "      <td>56840</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>99.92</td>\n",
       "      <td>75.25</td>\n",
       "      <td>78.35</td>\n",
       "      <td>76.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v5 testing</td>\n",
       "      <td>88</td>\n",
       "      <td>56772</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>99.82</td>\n",
       "      <td>87.13</td>\n",
       "      <td>49.72</td>\n",
       "      <td>63.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>v6 testing</td>\n",
       "      <td>82</td>\n",
       "      <td>56841</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>99.93</td>\n",
       "      <td>81.19</td>\n",
       "      <td>80.39</td>\n",
       "      <td>80.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 309
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
