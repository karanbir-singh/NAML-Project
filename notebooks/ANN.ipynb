{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T23:46:57.300168Z",
     "start_time": "2025-01-26T23:46:56.735766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.oversampling import fit_resample\n",
    "from utils.data_processer import *"
   ],
   "id": "57daa5998f08eb3a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "bd778b3540b7a063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T23:47:11.919744Z",
     "start_time": "2025-01-26T23:47:11.120735Z"
    }
   },
   "source": [
    "data = pd.read_csv(\"../creditcard_2021.csv\")\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "print(f\"Number of fraudolent transaction: {(data['Class'] == 1).sum()}\")\n",
    "print(f\"Ratio of fraudolent transaction: {data['Class'].mean()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 284807\n",
      "Number of fraudolent transaction: 492\n",
      "Ratio of fraudolent transaction: 0.001727485630620034\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d39d31c02285bb41"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-27T11:18:14.126924Z",
     "start_time": "2025-01-27T11:18:14.112342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ANN:\n",
    "    def __init__(self, layers_size=None, act_func=jnp.tanh, out_act_fun = jax.nn.sigmoid):\n",
    "        self.layers_size = layers_size\n",
    "        self.act_func = act_func \n",
    "        self.out_act_fun = out_act_fun\n",
    "\n",
    "    def initialize_parameters(self, layers_size=None):\n",
    "        \"\"\"\n",
    "            Sets the parameters of the artificial neural network given the number of\n",
    "            neurons of its layers, namely it sets the matrix of weights and the bias\n",
    "            vector for each layer \n",
    "          \n",
    "            Parameters:\n",
    "            layers_size: list - ordered sizes of the layers of the artificial neural network\n",
    "            \n",
    "            Raises:\n",
    "            exception: if layers_size is not provided\n",
    "        \"\"\"\n",
    "\n",
    "        if layers_size is None:\n",
    "            raise Exception(\"Size of layers not provided\")\n",
    "        \n",
    "        layers_size = jnp.array(layers_size)\n",
    "        \n",
    "        np.random.seed(0) # For reproducibility\n",
    "        self.layers_size = layers_size\n",
    "        params = list()\n",
    "        \n",
    "        for i in range(len(self.layers_size) - 1):\n",
    "            W = np.random.randn(self.layers_size[i+1], self.layers_size[i])\n",
    "            b = np.zeros((self.layers_size[i+1], 1))\n",
    "            params.append(W)\n",
    "            params.append(b)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def MSW(self, params):\n",
    "        \"\"\"\n",
    "        Computes the sum of the squared values of the weights of the artificial neural\n",
    "        network\n",
    "    \n",
    "        Parameters:\n",
    "        params: list - parameters of the artificial neural network, namely weights and biases\n",
    "        \n",
    "        Returns:\n",
    "        float - sum of the squared values of the weights of the artificial neural network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract weights\n",
    "        weights = params[::2]\n",
    "        \n",
    "        # Calculate MSW\n",
    "        partial_sum = 0.0\n",
    "        n_weights = 0\n",
    "        for W in weights:\n",
    "            partial_sum = partial_sum + jnp.sum(W * W)\n",
    "            n_weights = n_weights + W.size\n",
    "            \n",
    "        return partial_sum / n_weights\n",
    "    \n",
    "    # Loss functions\n",
    "    def cross_entropy(self):\n",
    "        @jax.jit\n",
    "        def callable(x=None, y=None, params=None):\n",
    "            \"\"\"\n",
    "                Computes the Cross Entropy Cost function\n",
    "                \n",
    "                Parameters:\n",
    "                x: ndarray - input of the artificial neural network\n",
    "                y: ndarray - correct value of the output, one-hot representation\n",
    "                \n",
    "                Returns:\n",
    "                float - Cross Entropy Cost between the predictions of the artificial neural network and the correct values\n",
    "                \n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if params are not provided\n",
    "            \"\"\"\n",
    "            \n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if params is None:\n",
    "                raise Exception(\"params are not provided\")\n",
    "        \n",
    "            y_pred = self.predict(x, params)\n",
    "            return -jnp.mean(y * jnp.log(y_pred) + (1 - y) * jnp.log(1 - y_pred))\n",
    "        return callable\n",
    "    \n",
    "    def mean_squared_error(self):\n",
    "        @jax.jit\n",
    "        def callable(x, y, params):\n",
    "            \"\"\"\n",
    "                Computes the Mean Squared Error\n",
    "                \n",
    "                Parameters:\n",
    "                x: ndarray - input of the artificial neural network\n",
    "                y: ndarray - correct value of the output, one-hot representation\n",
    "                \n",
    "                Returns:\n",
    "                float - Mean Squared Error between the predictions of the artificial neural network and the correct values\n",
    "                \n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if params are not provided\n",
    "            \"\"\"\n",
    "            \n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if params is None:\n",
    "                raise Exception(\"params are not provided\")\n",
    "            \n",
    "            y_pred = self.predict(x, params)\n",
    "            return jnp.mean((y_pred - y) ** 2)\n",
    "        return callable\n",
    "    \n",
    "    def loss_regularization(self, loss_function, penalization):\n",
    "        @jax.jit\n",
    "        def callable(x=None, y=None, params=None):\n",
    "            \"\"\"\n",
    "                Computes the loss function applying regularization to the given loss function with penalization \n",
    "            \n",
    "                Parameters:\n",
    "                x: ndarray - input of the artificial neural network\n",
    "                y: ndarray - correct value of the output\n",
    "                params: list - parameters of the artificial neural network, namely weights and biases\n",
    "                penalization: float - weight to which the MSW is multiplied and \n",
    "                                    that makes possible to modify the impact of the regularization term\n",
    "                Returns:\n",
    "                float - loss function value between the predictions of the artificial neural network\n",
    "                        and the correct values with regularization term\n",
    "              \n",
    "                Raises:\n",
    "                exception: if x is not provided\n",
    "                exception: if y is not provided\n",
    "                exception: if params are not provided\n",
    "            \"\"\"\n",
    "                \n",
    "            if x is None:\n",
    "                raise Exception(\"x is not provided\")\n",
    "            if y is None:\n",
    "                raise Exception(\"y is not provided\")\n",
    "            if params is None:\n",
    "                raise Exception(\"params are not provided\")\n",
    "            \n",
    "            return loss_function(x, y, params) + penalization/(2 * x.shape[0]) * self.MSW(params)\n",
    "        return callable\n",
    "    \n",
    "    # Optimisation algorithms\n",
    "    def SGD(\n",
    "            self, \n",
    "            loss_function, \n",
    "            epochs=1000, \n",
    "            batch_size=128, \n",
    "            learning_rate_min=1e-3, \n",
    "            learning_rate_max=1e-1, \n",
    "            learning_rate_decay=1000,\n",
    "            \n",
    "    ):\n",
    "        def callable(x_train, y_train, params):\n",
    "            \"\"\"\n",
    "                Stochastic gradient descent method with mini-batch and learning rate decay\n",
    "            \n",
    "                Parameters:\n",
    "                x_train: ndarray - training set of the dataset to fit\n",
    "                y_train: ndarray - training set's sample's labels\n",
    "                params: list - parameters of the artificial neural network, namely weights and biases\n",
    "                epochs: int - number of epochs to perform\n",
    "                batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "                learning_rate_min: float - minimum learning rate used in the training phase\n",
    "                learning_rate_max: float - maximum learning rate used in the training phase\n",
    "                learning_rate_decay: float - learning rate decay used in the training phase\n",
    "            \n",
    "                Returns:\n",
    "                parameters: list - trained parameters of the artificial neural network, \n",
    "                                    namely weights and biases optimized for fitting the training set\n",
    "            \"\"\"\n",
    "    \n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "        \n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=2))\n",
    "        \n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, params))\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Get learning rate\n",
    "                learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
    "        \n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "                \n",
    "                # Calculate gradient\n",
    "                grad_val = grad_loss(x_train[idxs,:], y_train[idxs,:], params)\n",
    "        \n",
    "                # Update params\n",
    "                for i in range(len(params)):\n",
    "                    params[i] = params[i] - learning_rate * grad_val[i]\n",
    "                \n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, params))\n",
    "            return params, history\n",
    "        return callable\n",
    "    \n",
    "    def SGD_momentum(\n",
    "            self, \n",
    "            loss_function, \n",
    "            epochs=1000, \n",
    "            batch_size=128, \n",
    "            learning_rate_min=1e-3, \n",
    "            learning_rate_max=1e-1, \n",
    "            learning_rate_decay=1000,\n",
    "            momentum=0.9,\n",
    "    ):\n",
    "        def callable(x_train, y_train, params):\n",
    "            \"\"\"\n",
    "                Stochastic gradient descent method with momentum, mini-batch and learning rate decay\n",
    "            \n",
    "                Parameters:\n",
    "                x_train: ndarray - training set of the dataset to fit\n",
    "                y_train: ndarray - training set's sample's labels\n",
    "                params: list - parameters of the artificial neural network, namely weights and biases\n",
    "                epochs: int - number of epochs to perform\n",
    "                batch_size: int, optional - size of the batches to be used for computing the gradient\n",
    "                learning_rate_min: float - minimum learning rate used in the training phase\n",
    "                learning_rate_max: float - maximum learning rate used in the training phase\n",
    "                learning_rate_decay: float - learning rate decay used in the training phase\n",
    "                momentum: float - momentum used in the training phase\n",
    "            \n",
    "                Returns:\n",
    "                parameters: list - trained parameters of the artificial neural network, \n",
    "                                    namely weights and biases optimized for fitting the training set\n",
    "            \"\"\"\n",
    "    \n",
    "            # Number of samples\n",
    "            num_samples = x_train.shape[0]\n",
    "        \n",
    "            # Loss and it's gradient functions\n",
    "            loss = jax.jit(loss_function)\n",
    "            grad_loss = jax.jit(jax.grad(loss_function, argnums=2))\n",
    "            \n",
    "            # History\n",
    "            history = list()\n",
    "            history.append(loss(x_train, y_train, params))\n",
    "            \n",
    "            # Initialize velocity\n",
    "            velocity = list()\n",
    "            for i in range(len(params)):\n",
    "                velocity.append(np.zeros_like(params[i]))\n",
    "                \n",
    "            for epoch in range(epochs):\n",
    "                # Get learning rate\n",
    "                learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
    "        \n",
    "                # Select batch_size indices randomly\n",
    "                idxs = np.random.choice(num_samples, batch_size)\n",
    "                \n",
    "                # Calculate gradient\n",
    "                grad_val = grad_loss(x_train[idxs,:], y_train[idxs,:], params)\n",
    "                    \n",
    "                for i in range(len(params)):\n",
    "                    # Compute velocity[i]\n",
    "                    velocity[i] = momentum * velocity[i] - learning_rate * grad_val[i]\n",
    "\n",
    "                    # Update params[i]\n",
    "                    params[i] = params[i] + velocity[i]\n",
    "                \n",
    "                # Update history\n",
    "                history.append(loss(x_train, y_train, params))\n",
    "            return params, history\n",
    "        return callable\n",
    "    \n",
    "    def train(self, x_train, y_train, params, opt_algorithm):\n",
    "        \"\"\"\n",
    "            Trains the artificial neural network using one the optimization algorithms\n",
    "        \n",
    "            Parameters:\n",
    "            x_train: ndarray - training set of the dataset to fit\n",
    "            y_train: ndarray - training set's sample's labels\n",
    "            params: ndarray - parameters of the artificial neural network, namely weights and biases\n",
    "            opt_algorithm: callable - optimization algorithm to be used in the training phase\n",
    "\n",
    "            Returns:\n",
    "            ndarray - updated weights and bias\n",
    "            ndarray - history of the loss function\n",
    "        \"\"\"\n",
    "        \n",
    "        return opt_algorithm(x_train, y_train, params)\n",
    "    \n",
    "    def predict(self, x=None, params=None):\n",
    "        \"\"\"\n",
    "            Computes the value of the output of the artificial neural network given an input\n",
    "        \n",
    "            Parameters:\n",
    "            x: ndarray - input of the artificial neural network\n",
    "            act_func: callable - activation function of the artificial neural network\n",
    "            out_act_fun: callable - output activation function of the artificial neural network\n",
    "        \n",
    "            Returns: \n",
    "            ndarray - output value of the artificial neural network\n",
    "                \n",
    "            Raises:\n",
    "            Exception - if x is not provided\n",
    "            Exception - if parameters were not initialized\n",
    "        \"\"\"\n",
    "        \n",
    "        if x is None:\n",
    "            raise Exception(\"x is not provided\")\n",
    "        if params is None:\n",
    "            raise Exception(\"Parameters were not initialized\")\n",
    "        \n",
    "        # Number of ANN layers\n",
    "        num_layers = int(len(self.layers_size)) + 1\n",
    "        \n",
    "        # Algorithm\n",
    "        layer = x.T\n",
    "        weights = params[0::2]\n",
    "        biases = params[1::2]\n",
    "        for i in range(num_layers - 2):\n",
    "            # Update layer values\n",
    "            layer = weights[i] @ layer + biases[i]\n",
    "            \n",
    "            # Apply activation function\n",
    "            layer = self.act_func(layer)\n",
    "                  \n",
    "        # On the output layer it is applied the sigmoid function \n",
    "        # since the output is needed to be between 0 and 1\n",
    "        layer = self.out_act_fun(layer)\n",
    "        layer = layer.T\n",
    "        \n",
    "        return layer"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:18:16.605874Z",
     "start_time": "2025-01-27T11:18:16.415443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "v5 = ['Time', 'V1', 'V7', 'V8', 'V9', 'V11', 'V12', 'V14', 'V15', 'V22', 'V27', 'V28', 'Amount', 'Class']\n",
    "input = data[v5].to_numpy()\n",
    "\n",
    "# Data splitting\n",
    "x_train, y_train, _, _, x_test, y_test = data_split(data_input=input)\n",
    "\n",
    "# Training set normalisation\n",
    "x_train_normalized, data_train_min, data_train_max = min_max(data=x_train)\n",
    "\n",
    "# SMOTE\n",
    "x_train_synthetic = fit_resample(x_train_normalized[y_train[:, 0] == 1], n_samples=50000)\n",
    "y_train_synthetic = np.ones((50000,1))\n",
    "\n",
    "x_train_normalized = np.concatenate((x_train_normalized, x_train_synthetic), axis=0)\n",
    "y_train = np.concatenate((y_train, y_train_synthetic), axis=0)"
   ],
   "id": "a8f9f4a0697c19f2",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:18:29.950686Z",
     "start_time": "2025-01-27T11:18:18.487509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ANN with:\n",
    "# activation function for each layer: ReLU\n",
    "# activation function for last layer: Sigmoid\n",
    "network = ANN(act_func=jnp.tanh, out_act_fun=jax.nn.sigmoid)\n",
    "\n",
    "# Initialize weights and biases\n",
    "params = network.initialize_parameters([x_train_normalized.shape[1], 30, 20, 1])\n",
    "\n",
    "# Train ann, with gradient descent - mean squared error\n",
    "updated_params, history = network.train(\n",
    "    x_train = x_train_normalized, \n",
    "    y_train = y_train, \n",
    "    params = params,    \n",
    "    opt_algorithm = network.SGD_momentum(\n",
    "        loss_function=network.cross_entropy()\n",
    "    )\n",
    ")"
   ],
   "id": "2fc53309be2ee77b",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:18:32.660084Z",
     "start_time": "2025-01-27T11:18:32.589561Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(history)",
   "id": "75f6eb5594ffb8a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x320871b80>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:18:44.674617Z",
     "start_time": "2025-01-27T11:18:41.431915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TP = network.predict(x_train_normalized[y_train[:, 0] == 1], updated_params)\n",
    "TN = 1 - network.predict(x_train_normalized[y_train[:, 0] == 0], updated_params)\n",
    "\n",
    "accuracy = (sum(TP) + sum(TN)) / x_train_normalized.shape[0]\n",
    "accuracy * 100"
   ],
   "id": "ce5fcfe542e536de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([64.87416], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
